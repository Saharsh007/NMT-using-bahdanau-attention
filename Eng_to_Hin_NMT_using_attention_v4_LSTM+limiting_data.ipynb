{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Eng_to_Hin_NMT_using_attention_v4_LSTM+limiting data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ngN8uPp_0pML"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGYU3Wh_0pIc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# NMT using attention\n",
        "## NOTICE : Attention layer is not yet implemented in keras library , which makes implementation of attention mechanism a lot difficult than other neural network models\n",
        "## We can't use keras's model class and do model.fit , instead we'll have to write optimizer, loss function, model layers and training code from scratch and combine them to create a workflow. This is equivalent to writing model.fit in normal neural network models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PUXXp2K0pIg",
        "colab_type": "code",
        "outputId": "07bddd66-524e-4e25-9110-b0bf51edcac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import string"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4_-K5Jzrx-W",
        "colab_type": "text"
      },
      "source": [
        "# loading the 1st dataset as it contains more smaller sentences which might help in better learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOTBKDZcr1et",
        "colab_type": "code",
        "outputId": "aaa54ced-3952-4137-cc81-1d74d1bd7019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# download dataset\n",
        "!wget 'http://www.manythings.org/anki/hin-eng.zip'\n",
        "!unzip 'hin-eng.zip'\n",
        "lines = io.open('hin.txt', encoding='UTF-8').read().strip().split('\\n')\n",
        "path_to_file = 'hin.txt'\n",
        "\n",
        "eng_sent = []\n",
        "hin_sent = []\n",
        "for line in lines:\n",
        "  splits = line.split('\\t')\n",
        "  #removing extra stuffs \n",
        "  eng = splits[0]\n",
        "  hin = splits[1]\n",
        "  # remove punctutation and make lower case\n",
        "  eng = eng.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = eng.lower()\n",
        "  hin = hin.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = '<start> ' + eng + ' <end>'\n",
        "  hin = '<start> ' + hin + ' <end>'\n",
        "  eng_sent.append(eng)\n",
        "  hin_sent.append(hin)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-19 20:34:10--  http://www.manythings.org/anki/hin-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:3037::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 126449 (123K) [application/zip]\n",
            "Saving to: ‘hin-eng.zip.1’\n",
            "\n",
            "\rhin-eng.zip.1         0%[                    ]       0  --.-KB/s               \rhin-eng.zip.1       100%[===================>] 123.49K   711KB/s    in 0.2s    \n",
            "\n",
            "2020-03-19 20:34:10 (711 KB/s) - ‘hin-eng.zip.1’ saved [126449/126449]\n",
            "\n",
            "Archive:  hin-eng.zip\n",
            "replace hin.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: hin.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em5rP-OalHSR",
        "colab_type": "text"
      },
      "source": [
        "## Download and process 2nd dataset\n",
        "## this has 7 lakhs translations of which we'll use 90k whose length are less than 50 words because of system's limitation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmkR-YLV0pIp",
        "colab_type": "code",
        "outputId": "bf671de0-fa8a-4392-cb46-9242f74b5870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# download dataset\n",
        "!wget 'http://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/iitb_corpus_download/prunedCorpus.tar.gz'\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-19 20:34:49--  http://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/iitb_corpus_download/prunedCorpus.tar.gz\n",
            "Resolving www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)... 103.21.127.130\n",
            "Connecting to www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)|103.21.127.130|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84265584 (80M) [application/x-gzip]\n",
            "Saving to: ‘prunedCorpus.tar.gz’\n",
            "\n",
            "prunedCorpus.tar.gz 100%[===================>]  80.36M  5.01MB/s    in 32s     \n",
            "\n",
            "2020-03-19 20:35:21 (2.55 MB/s) - ‘prunedCorpus.tar.gz’ saved [84265584/84265584]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "562vbIWW0pIw",
        "colab_type": "code",
        "outputId": "27f161a1-13bf-4bf8-b02a-3cfc36ee6c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!ls\n",
        "# !unzip 'prunedCorpus.tar.gz'\n",
        "!tar -xvzf 'prunedCorpus.tar.gz'\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_about.txt   hin-eng.zip.1  prunedCorpus.tar.gz\n",
            "hin-eng.zip  hin.txt\t    sample_data\n",
            "pruned_train.en\n",
            "pruned_train.hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoPVHC4ZjduR",
        "colab_type": "code",
        "outputId": "282404a1-6341-4303-f73b-e4aa2b796048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_about.txt   hin-eng.zip.1  prunedCorpus.tar.gz  pruned_train.hi\n",
            "hin-eng.zip  hin.txt\t    pruned_train.en\t sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9ckuHLOfIv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines_hin = io.open('pruned_train.hi', encoding='UTF-8').read().strip().split('\\n')\n",
        "path_to_file = 'pruned_train.hi'\n",
        "\n",
        "lines_eng = io.open('pruned_train.en', encoding='UTF-8').read().strip().split('\\n')\n",
        "path_to_file = 'pruned_train.en'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGmDgx3efKu6",
        "colab_type": "code",
        "outputId": "b320fede-2cea-4d02-d137-2bcc7dfa50ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "for i in range(5):\n",
        "  print(lines_eng[i],lines_hin[i])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We generally find this capacity being directed towards mundane objects and close relations like wives , children and friends .  प्रायः हम इस भंडार को लौकिक विषयों पर और पत्नी , पुत्र मित्र आदि सगे संबंधियों पर प्रवर्तित कर देते हैं । \n",
            "I ' m sure I ' ll make a careless mistake मुझे जनता हूँ की मैं इस वीडियो में किसी भी समय\n",
            "Display web browser help वेब ब्राउज़र मदद दिखाएँ\n",
            "It is out of His Mercy that He has made for you Night and Day , - that ye may rest therein , and that ye may seek of his Grace ; - and in order that ye may be grateful .  और उसने अपनी मेहरबानी से तुम्हारे वास्ते रात और दिन को बनाया ताकि तुम रात में आराम करो और दिन में उसके फज़ल व करम की तलाश करो और ताकि तुम लोग शुक्र करो\n",
            "The disease caused by nutritional deficiency of ascorbic acid , formaly occuring in people who had not been to sea results in circular spots , stripes or patches scattered over thighs , arms , and trunk .  एब्सकार्बिक अम्ल कुपोषण की कमी के कारण द्वारा होने वाली बीमारी जो आमतौर से लोगो में होता है जिसके कारण गोलाकार धब्बे , पट्टियों या जांघों , बांह और धड़ पर चकत्ते । \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZxcav5VlMsN",
        "colab_type": "code",
        "outputId": "118e7ed4-22cb-4df0-9f62-977b28433834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(lines_eng),len(lines_hin))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "788098 788098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5OXf1td0pI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(lines_hin)):\n",
        "  # splits = line.split('\\t')\n",
        "  #removing extra stuffs \n",
        "  eng = lines_eng[i]\n",
        "  hin = lines_hin[i]\n",
        "  # remove punctutation and make lower case\n",
        "  eng = eng.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = eng.lower()\n",
        "  hin = hin.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = '<start> ' + eng + ' <end>'\n",
        "  hin = '<start> ' + hin + ' <end>'\n",
        "  if len(eng) <= 50:\n",
        "    eng_sent.append(eng)\n",
        "  if len(hin) <= 50:\n",
        "    hin_sent.append(hin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mh6mOtu0pJd",
        "colab_type": "code",
        "outputId": "4844d1b7-4075-4d3b-aaf5-59437e9e8343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(eng_sent[1:10],hin_sent[1:10])\n",
        "print(len(eng_sent))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<start> help <end>', '<start> jump <end>', '<start> jump <end>', '<start> jump <end>', '<start> hello <end>', '<start> hello <end>', '<start> cheers <end>', '<start> cheers <end>', '<start> got it <end>'] ['<start> बचाओ <end>', '<start> उछलो <end>', '<start> कूदो <end>', '<start> छलांग <end>', '<start> नमस्ते। <end>', '<start> नमस्कार। <end>', '<start> वाहवाह <end>', '<start> चियर्स <end>', '<start> समझे कि नहीं <end>']\n",
            "119794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh69CycGraxP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O4W676E8-_Y",
        "colab_type": "text"
      },
      "source": [
        "# code to encode dataset and create dictionary using keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRwYu__H0pJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdkAsOUD0pJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    # default filter value is '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    # an instance of tokenizer\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "    \n",
        "    #takes all sentences in lang and makes dictionary for it\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    \n",
        "    #encodes the text -> (text ->vector)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    #puts appropriate number of zeros after the sent\n",
        "    # if maxlen of sentences if 100 and any given sentence is of len 20, then\n",
        "    # it'll pad 80 zeros at the end\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "    # tensor is a vector of n*maxlen\n",
        "    # and lang_tokenizer is a dictionary mapping word with key \n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSlzBh30pJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YfOYE-k0pJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(inp_lang,targ_lang,load_dataset):\n",
        "    inp_lang = inp_lang[0:num_examples-1]\n",
        "    targ_lang = targ_lang[0:num_examples-1]\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mr8IlOCbcdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLmet1lA0pJ5",
        "colab_type": "text"
      },
      "source": [
        "# encoding datatset and creating dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvkZM620pJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "# num_examples = 20000\n",
        "num_examples = len(eng_sent)\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(eng_sent,hin_sent,num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVhz5n6i0pKC",
        "colab_type": "code",
        "outputId": "c2499157-2e54-4a82-bcd3-2a8b7e403550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(input_tensor.shape,target_tensor.shape)\n",
        "print(\"max length of input and output are : \", max_length_inp,max_length_targ )\n",
        "print('english dictionary is : ')\n",
        "print(inp_lang)\n",
        "print(\"hindi dictionary is : \")\n",
        "print(targ_lang)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(119793, 24) (119793, 27)\n",
            "max length of input and output are :  24 27\n",
            "english dictionary is : \n",
            "<keras_preprocessing.text.Tokenizer object at 0x7fecfa01ee10>\n",
            "hindi dictionary is : \n",
            "<keras_preprocessing.text.Tokenizer object at 0x7fecfa01ee48>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSTMdEJF0pKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05i8WUQT0pKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ucRpKUo0pKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrMEbrgJ0pKZ",
        "colab_type": "code",
        "outputId": "8be1afc9-ff2e-4701-a460-b3a91be4e9e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.3)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83855 83855 35938 35938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayaYd_fn0pKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #This is what our model is going to learn\n",
        "# def convert(lang, tensor):\n",
        "#     for t in tensor:\n",
        "#         if t!=0:\n",
        "#             print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "# print (\"Input Language; index to word mapping\")\n",
        "# convert(inp_lang, input_tensor_train[0])\n",
        "# print ()\n",
        "# print (\"Target Language; index to word mapping\")\n",
        "# convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRmaikr_0pKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2dB-PgW0pKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train) #training set size\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "#Creates a Dataset whose elements are slices of the given tensors.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "#Combines consecutive elements of this dataset into batches.\n",
        "#eg dataset = tf.data.Dataset.range(8) \n",
        "# dataset = dataset.batch(3) \n",
        "# list(dataset.as_numpy_iterator()) \n",
        "# [ array([0,1,2]), array([3,4,5]) , array([5,6,7])]\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2ZwrCC0pKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i7_WbqN0pK0",
        "colab_type": "code",
        "outputId": "ae4d16b0-bc3c-47c9-f7ad-e9fc4ac8edd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# so dataset loads 64 examples at a time for both input and output\n",
        "# its a pointer to point to next batch when needed.\n",
        "print(dataset)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((32, 24), (32, 27)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UaxPkM50pK6",
        "colab_type": "code",
        "outputId": "21e93727-3f40-417c-e51d-8fbab020d897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(32), Dimension(24)]),\n",
              " TensorShape([Dimension(32), Dimension(27)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmPlb5vH0pK_",
        "colab_type": "code",
        "outputId": "b44e8977-2cdb-4ab4-ee5a-d285a986a7fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(example_input_batch)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[    1  1280   184  8267   127    93     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    17    18     6  7096 26104     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    43    11    80  1486     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1  7624   388   748     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   276  2857    10  9402  2157     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    17    16    11     6 11015  5698  2508     2     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    13   572    45    96  4006     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    38     4     3   137     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   136  3136  2492     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    63  3415   360     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    44    45    28    74   597     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   121     8   253   813     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    39    12    21   263    68     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1  9744     4   128     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   120     6    35   170     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1     3   168     4     3  2958     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    39   200    87    27  5491     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1     5  7971    10  2815    18     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1     3   170    20     5    48     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   159   360   229     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   848  4700  2716  6236     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    63  1768   129     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    43    11   240   657   293    60     2     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    39    42  1220  1005  2167     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    18    24   266  4177    40 11467   377     2     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   137     3   166  1227     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   345     4 12566  7906     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   106    17   944     9  7257     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1  8909   334   265     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1    83    37    21    42  1086    10   104    83     2     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   247    33   524   803  1575     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   383   112   119     7  1201     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]], shape=(32, 24), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frqAhmQV0pLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuLqs00r0pLK",
        "colab_type": "text"
      },
      "source": [
        "# Making the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmRIFAol0pLR",
        "colab_type": "text"
      },
      "source": [
        "**return_sequences** -> give hidden state for each time step  \n",
        "**return_state** -> gives (in case of lstm) [hidden state, hidden state, cell state for last time step].  \n",
        "**both of them** -> (in case of lstm)[hidden state for all time step, hidden state for last time step, cell state for last time step]  \n",
        "(in case of gru)[hidden state for all time steps, hidden state for final timestep]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Ygwhtw0pLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        #takes input and returns h and y\n",
        "        x = self.embedding(x)\n",
        "        output, state ,cell = self.lstm(x, initial_state = hidden)\n",
        "        encoder_states = [state,cell]\n",
        "        return output, encoder_states\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        state =  tf.zeros((self.batch_sz, self.enc_units))\n",
        "        return [state,state]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJzLv2P00pLZ",
        "colab_type": "code",
        "outputId": "d61a1546-2b84-43ae-bdbc-1d5a23f1f199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(vocab_inp_size)\n",
        "print(embedding_dim,units,BATCH_SIZE)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27488\n",
            "256 1024 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddGof_wu0pLf",
        "colab_type": "code",
        "outputId": "918f3fb6-3262-450f-9ba0-ad807002ecf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# encoder is a class instance having \n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# not sure how this line is working..\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "sample_output, encoder_states = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden[0].shape))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (32, 24, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (32, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxbfkV_l0pLl",
        "colab_type": "code",
        "outputId": "b5a7eb90-55db-433e-b2b7-a9ede28c1b7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "print(\"shape of sample_hidden\",sample_hidden[0].get_shape())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of sample_hidden (32, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF56_kQJ0pLp",
        "colab_type": "text"
      },
      "source": [
        "# ATTENTION MECHANISAM  CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe_AOor30pLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        \n",
        "        query = query[0]\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size) \n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVjh8iKZ0pL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu8Fx0Iw0pL9",
        "colab_type": "code",
        "outputId": "2c4b625c-83d3-4bd1-b699-4ca66efd78d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# this is just testing if attention is working or not\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (32, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (32, 24, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq0lelzP0pMB",
        "colab_type": "code",
        "outputId": "be399477-fda8-43e5-b226-d5ce369b05fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(sample_hidden)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: id=20, shape=(32, 1024), dtype=float32, numpy=\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: id=20, shape=(32, 1024), dtype=float32, numpy=\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Rh02IR0pMG",
        "colab_type": "text"
      },
      "source": [
        "## DECODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7HYsCtT0pMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        # enc_output is the list of all the hidden states\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "\n",
        "        output, hidden, cell = self.lstm(x)\n",
        "        decoder_states = [hidden,cell]\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, decoder_states, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngN8uPp_0pML",
        "colab_type": "text"
      },
      "source": [
        "### state is hidden state of gru for that time step\n",
        "### output and state are the same thing here , as this is only for one time step\n",
        "### output is :  Tensor(\"decoder_2/gru_5/transpose_1:0\", shape=(32, 1, 1024), dtype=float32)\n",
        "### state is :  Tensor(\"decoder_2/gru_5/while/Exit_3:0\", shape=(32, 1024), dtype=float32)\n",
        "notice the difference in shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE_RH93Q0pMN",
        "colab_type": "text"
      },
      "source": [
        "### testing if decoder class is working\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfVtqV_o0pMP",
        "colab_type": "code",
        "outputId": "c0fbd167-ffe2-467d-b377-275fc8e5713d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (32, 36647)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYK3knvn0pMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQQ7D8ZuA2bQ",
        "colab_type": "text"
      },
      "source": [
        "# OPTIMIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfa3quU-0pMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "'''\n",
        "In the snippet below, there is a single floating point value per example for\n",
        "`y_true` and `# classes` floating pointing values per example for `y_pred`.\n",
        "The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n",
        "`[batch_size, num_classes]`.\n",
        "'''\n",
        "#Computes the crossentropy loss between the labels and predictions.\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "# only considering cross entropy for incorrectly classified samples.\n",
        "def loss_function(real, pred):\n",
        "    # tf.math.logical - Returns the truth value of NOT x element-wise.\n",
        "    # tf.math.equal - Returns the truth value of (x == y) element-wise.\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    # loss as defined in the image above\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zpVkGjN0pMf",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6P1MX520pMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B241DBs60pMl",
        "colab_type": "code",
        "outputId": "3c694fd7-74d4-44af-e0d2-6c4c54257567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for (batch, (inp, targ)) in enumerate( dataset.take(steps_per_epoch) ):\n",
        "    print(targ)\n",
        "    print(targ.shape)\n",
        "    break"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[    1   122  6191 19784    17   408     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  1035    17  6195     3     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1 18352   980   855   953     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  2975     5    61     5    15     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1    52   318   348     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1 15045  2994 15046     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1 13068  8306    51     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1    44     9  1284    20    91    61  4821   202     2     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  4587  1844   306     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  2973  1249    12   184    68     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  1538    10 34676     5   821     7   542     2     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1   772   959  1937    13     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  3381   335    47   360  9517     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1   189 14730 14165   458     3     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  1885  1322 10833     7  7143     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  3218     5   212 34234  6859    90     4     3     2     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  3492    82     5   215  1935     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1 18800  9137    29 28929     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1 10694    11   309    49    33     4     2     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1    85  9974    17 13123     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1    14  4657    98     9    37   340    33     4     2     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  8975 30994 30995     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1    34   961    10  2449    17  2598     4     2     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1    45  3178   607     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1    20   813   153     4     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  2397  7924  2756     2     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1    19  4879  1046   473   300   174     4     2     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1   164     9   164    83    94    31   508    16     2     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1    74  1237   659   266     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1   444   112   829    18    32     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1  4254   665  1844   304     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    1   112  1065     6    67     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0]], shape=(32, 27), dtype=int32)\n",
            "(32, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hezjeujC0pMs",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daxa8Bz60pM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    '''\n",
        "    inp is encoded input sentence \n",
        "    targ is encoded output sentence(batchsize,len_of_target_word)\n",
        "    enc_hidden is zero vector of shape (batchsize,units)\n",
        "    '''\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        #recoder or tracks variable values\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # targ_lang.word_index['<start>'] - returns dictionary value of start token\n",
        "        # dec_input is batch_size\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            #enc_output is list of all hidden states of encoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            \n",
        "            # this is simply the loss between t th predicted word and its prediction\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            # expand_dims is to change dimention such that it can act as timesteps\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    \n",
        "    # explanationn of this part https://www.tensorflow.org/guide/effective_tf2\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    \n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NROWVZyG0pM6",
        "colab_type": "text"
      },
      "source": [
        "## operation code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMWEDCKS0pM7",
        "colab_type": "code",
        "outputId": "5fb59ec1-9882-4bcb-bd48-09f051488bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "EPOCHS = 15\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # returns a zero vector of shape(batch_size,units)\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    # dataset is tf.data.Dataset() object.\n",
        "    for (batch, (inp, targ)) in enumerate( dataset.take(steps_per_epoch) ):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MvHbnXxOzUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucOohbQEO1jd",
        "colab_type": "text"
      },
      "source": [
        "## Evaluator\n",
        "The evaluate function is similar to the training loop, except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA4CqPHaD6gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(eng):\n",
        "  eng = eng.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = eng.lower()\n",
        "  eng = '<start> ' + eng + ' <end>'\n",
        "  return eng"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZfulsHr0pNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  # this is for the heat map\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  #convert input to its embeddings\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_inp,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # we'll keep appending the predicted word in this\n",
        "  result = ''\n",
        "\n",
        "  # got all the hidden states and last encoder hidden state\n",
        "  hidden = [tf.zeros((1, units)),tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # for first timestep decoder hidden state = encoder hidden state\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  # keep feeding words in decoder for no of words in target sentence\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                          dec_hidden,\n",
        "                                                          enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    # max value from probability of all words in target sentence\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    # print(predictions.shape,predictions)\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jauhunjBZ_V_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stinglXdax8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSjdil8ta9JD",
        "colab_type": "text"
      },
      "source": [
        "# Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAV5x7eAa3Uq",
        "colab_type": "code",
        "outputId": "7f659313-310e-4aeb-b13d-05c59351d775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fae7a7f47f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf3Fur9GbCp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrJ4hztybDqP",
        "colab_type": "text"
      },
      "source": [
        "## checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBTdRi0WEUt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8vEmQFa49V",
        "colab_type": "code",
        "outputId": "3e709ed2-8a40-4420-d54c-ddb253405019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate('I will get back to you')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> i will get back to you <end>\n",
            "Predicted translation: मैं तुम्हारे साथ आऊँगा। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkiV_y9ebHEx",
        "colab_type": "code",
        "outputId": "5cda8216-88b7-47ad-9ecf-51646ba3fd2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate('This cat is nice')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> this cat is nice <end>\n",
            "Predicted translation: यह लिफ़्ट छटी मंज़िल तक ही है। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40qIl6PXCUbk",
        "colab_type": "code",
        "outputId": "97d68677-96e0-4cdc-ec12-fd3901cb9fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(u'i will not come')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> i will not come <end>\n",
            "Predicted translation: मैं तुम्हारे साथ आऊँगा। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RrASOTFHNeG",
        "colab_type": "code",
        "outputId": "92a1e241-c43f-40cc-a596-489fc6348ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(u'men are playing')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> men are playing <end>\n",
            "Predicted translation: वे हमारे साथ आएगा। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYd2qzwVHUuM",
        "colab_type": "code",
        "outputId": "26876bab-7daa-4836-8422-d374bc773487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(u'school is closed')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> school is closed <end>\n",
            "Predicted translation: स्कूल अप्रैल में बोलिए। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4YDk2xrHimX",
        "colab_type": "code",
        "outputId": "29548c3a-6a54-464f-f2ae-466c3bd72f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(u'why does this happen to me')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> why does this happen to me <end>\n",
            "Predicted translation: तुम क्यों नहीं हो <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St8LLzV2PsgI",
        "colab_type": "code",
        "outputId": "76e81fc4-987b-4dc2-e71d-c3a051679524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate('this is not working')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> this is not working <end>\n",
            "Predicted translation: यह भी यकीन है। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyovofYfPzlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}