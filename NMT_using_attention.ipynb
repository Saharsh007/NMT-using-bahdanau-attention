{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NMT using attention.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PuLqs00r0pLK",
        "UY0ME3ta0pLO",
        "AhYgC9v10pLd",
        "VF56_kQJ0pLp",
        "AafCX4sS0pLq",
        "m8Rh02IR0pMG",
        "ngN8uPp_0pML",
        "CE_RH93Q0pMN",
        "xkOkPxK60pMa",
        "2zpVkGjN0pMf",
        "hezjeujC0pMs"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGYU3Wh_0pIc",
        "colab_type": "text"
      },
      "source": [
        "# NMT using attention\n",
        "## NOTICE : Attention layer is not yet implemented in keras library , which makes implementation of attention mechanism a lot difficult than other neural network models\n",
        "## We can't use keras's model class and do model.fit , instead we'll have to write optimizer, loss function, model layers and training code from scratch and combine them to create a workflow. This is equivalent to writing model.fit in normal neural network models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PUXXp2K0pIg",
        "colab_type": "code",
        "outputId": "fddefa8a-c489-4919-dfa9-608b96c652a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmkR-YLV0pIp",
        "colab_type": "code",
        "outputId": "c89ff89d-6482-4472-d75b-caeacc7f10d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "562vbIWW0pIw",
        "colab_type": "code",
        "outputId": "95feaa42-7137-4fce-d75f-900b2092e371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(path_to_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/spa-eng/spa.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5OXf1td0pI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qp5FKQH0pI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "    \n",
        "#     print(\"1 -> \",w)\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "#     print(\"2-> \",w)\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
        "    \n",
        "#     print(\"3 -> \",w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "#     print(\"4 -> \",w)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
        "    \n",
        "#     print(\"5 -> \",w)\n",
        "    # rstrip() Remove spaces to the right of the string:\n",
        "    # strip() removes all leading and tailing whitespaces\"\n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "#     print(\"6 -> \",w)\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the m\"odel know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1fSDNYl0pJD",
        "colab_type": "code",
        "outputId": "8ec701ff-fba8-4739-a326-12e69a9d12b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"多Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "spanish = (preprocess_sentence(sp_sentence))\n",
        "print(spanish.encode('utf-8'))\n",
        "print(spanish)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n",
            "<start> 多 puedo tomar prestado este libro ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1lctWuV0pJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMro2SQz0pJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "    \n",
        "    # io.open() Open file and return a corresponding stream.\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    # w = [eng_sent,spanish_sent]\n",
        "    # l = [['eng \\t spanish sent'] , ['eng spanish sent']]\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48AAmJG30pJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mh6mOtu0pJd",
        "colab_type": "code",
        "outputId": "5f7e8120-21b2-496e-8ebc-fed3d911dae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRwYu__H0pJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdkAsOUD0pJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    # default filter value is '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    # an instance of tokenizer\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "    \n",
        "    #takes all sentences in lang and makes dictionary for it\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    \n",
        "    #encodes the text -> (text ->vector)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    #puts appropriate number of zeros after the sent\n",
        "    # if maxlen of sentences if 100 and any given sentence is of len 20, then\n",
        "    # it'll pad 80 zeros at the end\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "    # tensor is a vector of n*maxlen\n",
        "    # and lang_tokenizer is a dictionary mapping word with key \n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSlzBh30pJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YfOYE-k0pJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    #Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    #encoded sentence and dictionary for the same, dictionary will be used in embedding layer\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mr8IlOCbcdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLmet1lA0pJ5",
        "colab_type": "text"
      },
      "source": [
        "## Limiting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvkZM620pJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 20000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVhz5n6i0pKC",
        "colab_type": "code",
        "outputId": "e0f08e25-4e17-4c7a-9e90-e2e8ec7157a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(targ_lang.word_index['<start>'])\n",
        "print(targ_lang.index_word)\n",
        "dict_ = dict(targ_lang.index_word)\n",
        "print(len(dict_))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "{1: '<start>', 2: '<end>', 3: '.', 4: 'i', 5: 'you', 6: '?', 7: 'tom', 8: 'is', 9: 'it', 10: 's', 11: 'a', 12: 'he', 13: 't', 14: 'the', 15: 'we', 16: 'm', 17: 'me', 18: 're', 19: 'that', 20: 'this', 21: 'to', 22: 'do', 23: 'are', 24: 'can', 25: 'my', 26: 'they', 27: 'was', 28: '!', 29: 'she', 30: 'don', 31: 'have', 32: 'your', 33: 'go', 34: 'what', 35: 'in', 36: 'not', 37: 'll', 38: 'like', 39: 'here', 40: 'on', 41: 'him', 42: 'let', 43: 'be', 44: 'did', 45: 'know', 46: 'come', 47: 'up', 48: 'am', 49: 'want', 50: 'how', 51: ',', 52: 'mary', 53: 'get', 54: 'who', 55: 'very', 56: 'now', 57: 'need', 58: 'has', 59: 'please', 60: 'no', 61: 'there', 62: 'help', 63: 'her', 64: 'love', 65: 'at', 66: 'see', 67: 'just', 68: 'out', 69: 'his', 70: 've', 71: 'got', 72: 'for', 73: 'where', 74: 'look', 75: 'stop', 76: 'one', 77: 'us', 78: 'good', 79: 'car', 80: 'too', 81: 'so', 82: 'all', 83: 'why', 84: 'will', 85: 'an', 86: 'home', 87: 'of', 88: 'with', 89: 'give', 90: 'back', 91: 'were', 92: 'keep', 93: 'take', 94: 'dog', 95: 'saw', 96: 'didn', 97: 'isn', 98: 'may', 99: 'happy', 100: 'stay', 101: 'won', 102: 'work', 103: 'hate', 104: 'must', 105: 'wait', 106: 'leave', 107: 'again', 108: 'likes', 109: 'down', 110: 'feel', 111: 'book', 112: 'try', 113: 'made', 114: 'eat', 115: 'right', 116: 'them', 117: 'still', 118: 'had', 119: 'time', 120: 'going', 121: 'does', 122: 'money', 123: 'call', 124: 'say', 125: 'lost', 126: 'came', 127: 'tell', 128: 'went', 129: 'well', 130: 'today', 131: 'old', 132: 'busy', 133: 'looks', 134: 'ask', 135: 'away', 136: 'loves', 137: 'job', 138: 'man', 139: 'bad', 140: 'everyone', 141: 'never', 142: 'some', 143: 'over', 144: 'pay', 145: 'mine', 146: 'from', 147: 'ready', 148: 'alone', 149: 'read', 150: 'wrong', 151: 'room', 152: 'live', 153: 'angry', 154: 'tired', 155: 'talk', 156: 'more', 157: 'make', 158: 'could', 159: 'about', 160: 'nice', 161: 'nobody', 162: 'off', 163: 'french', 164: 'house', 165: 'hurt', 166: 'speak', 167: 'watch', 168: 'should', 169: 'left', 170: 'cold', 171: 'big', 172: 'late', 173: 'play', 174: 'new', 175: 'true', 176: 'and', 177: 'drink', 178: 'life', 179: 'our', 180: 'drunk', 181: 'lot', 182: 'open', 183: 'said', 184: 'these', 185: 'wasn', 186: 'boston', 187: 'way', 188: 'fast', 189: 'boy', 190: 'nothing', 191: 'turn', 192: 'understand', 193: 'ok', 194: 'hungry', 195: 'wants', 196: 'when', 197: 'really', 198: 'ate', 199: 'hurry', 200: 'died', 201: 'hot', 202: 'felt', 203: 'think', 204: 'name', 205: 'find', 206: 'sick', 207: 'yours', 208: 'everybody', 209: 'sit', 210: 'gave', 211: 'smart', 212: 'answer', 213: 'broke', 214: 'aren', 215: 'been', 216: 'hold', 217: 'miss', 218: 'stupid', 219: 'fun', 220: 'done', 221: 'sleep', 222: 'everything', 223: 'already', 224: 'listen', 225: 'fine', 226: 'bed', 227: 'first', 228: 'much', 229: 'sing', 230: 'loved', 231: 'coming', 232: 'great', 233: 'bought', 234: 'two', 235: 'father', 236: 'long', 237: 'cry', 238: 'married', 239: 'cat', 240: 'best', 241: 'tv', 242: 'hear', 243: 'friend', 244: 'ran', 245: 'stand', 246: 'crazy', 247: 'knows', 248: 'mad', 249: 'swim', 250: 'close', 251: 'idea', 252: 'easy', 253: 'rich', 254: 'hard', 255: 'walk', 256: 'start', 257: 'kill', 258: 'run', 259: 'put', 260: 'knew', 261: 'both', 262: 'doctor', 263: 'needs', 264: 'something', 265: 'almost', 266: 'wife', 267: 'only', 268: 'hair', 269: 'show', 270: 'by', 271: 'sad', 272: 'move', 273: 'remember', 274: 'dead', 275: 'gone', 276: 'crying', 277: 'drive', 278: 'free', 279: 'write', 280: 'school', 281: 'pretty', 282: 'those', 283: 'doesn', 284: 'trust', 285: 'tall', 286: 'd', 287: 'better', 288: 'friends', 289: 'fat', 290: 'sat', 291: 'bus', 292: 'bit', 293: 'heard', 294: 'water', 295: 'lie', 296: 'early', 297: 'called', 298: 'die', 299: 'hand', 300: 'met', 301: 'yourself', 302: 'bag', 303: 'teacher', 304: 'eyes', 305: 'bring', 306: 'lying', 307: 'yes', 308: 'day', 309: 'door', 310: 'believe', 311: 'doing', 312: 'enemy', 313: 'mean', 314: 'soon', 315: 'dogs', 316: 'wine', 317: 'buy', 318: 'as', 319: 'young', 320: 'monday', 321: 'singing', 322: 'red', 323: 'helped', 324: 'looked', 325: 'afraid', 326: 'someone', 327: 'quit', 328: 'guys', 329: 'use', 330: 'follow', 331: 'food', 332: 'found', 333: 'lucky', 334: 'key', 335: 'lunch', 336: 'tomorrow', 337: 'coffee', 338: 'kind', 339: 'hit', 340: 'safe', 341: 'forget', 342: 'study', 343: 'later', 344: 'whose', 345: 'seen', 346: 'enough', 347: 'running', 348: 'small', 349: 'son', 350: 'always', 351: 'enjoy', 352: 'might', 353: 'hope', 354: 'awake', 355: 'shot', 356: 'told', 357: 'fish', 358: 'arrived', 359: 'hat', 360: 'break', 361: 'short', 362: 'took', 363: 'quiet', 364: 'next', 365: 'works', 366: 'eating', 367: 'night', 368: 'asked', 369: 'cats', 370: 'milk', 371: 'seems', 372: 'dream', 373: 'dinner', 374: 'watching', 375: 'mother', 376: 'began', 377: 'thanks', 378: 'cool', 379: 'care', 380: 'real', 381: 'hurts', 382: 'cook', 383: 'check', 384: 'beer', 385: 'smoke', 386: 'hide', 387: 'fired', 388: 'rain', 389: 'send', 390: 'changed', 391: 'myself', 392: 'talking', 393: 'fault', 394: 'which', 395: 'hands', 396: 'yet', 397: 'brave', 398: 'full', 399: 'around', 400: 'liked', 401: 'problem', 402: 'win', 403: 'seat', 404: 'joking', 405: 'broken', 406: 'secret', 407: 'rest', 408: 'books', 409: 'hates', 410: 'english', 411: 'owe', 412: 'girl', 413: 'family', 414: 'would', 415: 'fell', 416: 'shy', 417: 'sorry', 418: 'along', 419: 'men', 420: 'kids', 421: 'naive', 422: 'reading', 423: 'working', 424: 'joke', 425: 'clean', 426: 'sure', 427: 'word', 428: 'little', 429: 'lied', 430: 'calm', 431: 'forgot', 432: 'warm', 433: 'patient', 434: 'missed', 435: 'begin', 436: 'snow', 437: 'needed', 438: 'woman', 439: 'lawyer', 440: 'beautiful', 441: 'happen', 442: 'kept', 443: 'lives', 444: 'child', 445: 'person', 446: 'tried', 447: 'shut', 448: 'fix', 449: 'failed', 450: 'hey', 451: 'promised', 452: 'anybody', 453: 'change', 454: 'funny', 455: 'once', 456: 'pen', 457: 'drank', 458: 'things', 459: 'plan', 460: 'own', 461: 'insane', 462: 'truth', 463: 'game', 464: 'born', 465: 'hang', 466: 'wake', 467: 'smiled', 468: 'cut', 469: 'boring', 470: 'empty', 471: 'kidding', 472: 'smell', 473: 'meet', 474: 'happened', 475: 'wish', 476: 'blue', 477: 'outside', 478: 'studying', 479: 'useless', 480: 'box', 481: 'somebody', 482: 'clever', 483: 'shoes', 484: 'fire', 485: 'moved', 486: 'perfect', 487: 'kiss', 488: 'laughed', 489: 'blind', 490: 'far', 491: 'worked', 492: 'dark', 493: 'thank', 494: 'anyone', 495: 'meat', 496: 'hiding', 497: 'alive', 498: 'white', 499: 'dance', 500: 'cheated', 501: 'confused', 502: 'black', 503: 'sweet', 504: 'cake', 505: 'japanese', 506: 'people', 507: 'raining', 508: 'minute', 509: 'keys', 510: 'breath', 511: 'idiot', 512: 'phone', 513: 'betrayed', 514: 'story', 515: 'hi', 516: 'agree', 517: 'cute', 518: 'save', 519: 'fly', 520: 'slowly', 521: 'tea', 522: 'single', 523: 'fight', 524: 'blame', 525: 'trapped', 526: 'party', 527: 'face', 528: 'rules', 529: 'music', 530: 'lonely', 531: 'last', 532: 'stopped', 533: 'explain', 534: 'asleep', 535: 'six', 536: 'guy', 537: 'brother', 538: 'any', 539: 'place', 540: 'started', 541: 'beat', 542: 'deep', 543: 'talked', 544: 'warn', 545: 'dying', 546: 'sign', 547: 'ice', 548: 'wrote', 549: 'count', 550: 'finish', 551: 'dumb', 552: 'often', 553: 'naked', 554: 'waiting', 555: 'anything', 556: 'became', 557: 'bath', 558: 'sang', 559: 'light', 560: 'listening', 561: 'touch', 562: 'head', 563: 'seem', 564: 'glasses', 565: 'sounds', 566: 'tie', 567: 'into', 568: 'couldn', 569: 'mouth', 570: 'being', 571: 'speaks', 572: 'slept', 573: 'welcome', 574: 'bald', 575: 'weak', 576: 'helps', 577: 'paid', 578: 'after', 579: 'inside', 580: 'cried', 581: 'forgive', 582: 'god', 583: 'survived', 584: 'moving', 585: 'sleepy', 586: 'green', 587: 'curious', 588: 'excited', 589: 'doll', 590: 'pain', 591: 'another', 592: 'arm', 593: 'losing', 594: 'fill', 595: 'gun', 596: 'comes', 597: 'tennis', 598: 'week', 599: 'advice', 600: 'important', 601: 'horse', 602: 'dry', 603: 'learn', 604: 'turned', 605: 'fishing', 606: 'canadian', 607: 'summer', 608: 'choice', 609: 'quite', 610: 'war', 611: 'cannot', 612: 'paper', 613: 'three', 614: 'smile', 615: 'fair', 616: 'catch', 617: 'weird', 618: 'ours', 619: 'careful', 620: 'serious', 621: 'hero', 622: 'liar', 623: 'normal', 624: 'trying', 625: 'cheese', 626: 'walked', 627: 'strong', 628: 'song', 629: 'shall', 630: 'sleeping', 631: 'horses', 632: 'student', 633: 'forgetful', 634: 'together', 635: 'second', 636: 'luck', 637: 'raise', 638: 'mistake', 639: 'clock', 640: 'trouble', 641: 'end', 642: 'getting', 643: 'noise', 644: 'maybe', 645: 'brown', 646: 'himself', 647: 'shirt', 648: 'okay', 649: 'poor', 650: 'doubt', 651: 'laugh', 652: 'strange', 653: 'news', 654: 'baffled', 655: 'cooking', 656: 'nervous', 657: 'worried', 658: 'mess', 659: 'invited', 660: 'quickly', 661: 'apples', 662: 'women', 663: 'thrilled', 664: 'writing', 665: 'closed', 666: 'boss', 667: 'thirsty', 668: 'deal', 669: 'satisfied', 670: 'worse', 671: 'duty', 672: 'simple', 673: 'cup', 674: 'also', 675: 'truck', 676: 'if', 677: 'throw', 678: 'killed', 679: 'apple', 680: 'letter', 681: 'sister', 682: 'teeth', 683: 'bicycle', 684: 'easily', 685: 'jump', 686: 'join', 687: 'wash', 688: 'waited', 689: 'ignore', 690: 'walks', 691: 'cruel', 692: 'bored', 693: 'town', 694: 'bread', 695: 'soup', 696: 'gas', 697: 'saved', 698: 'many', 699: 'kissed', 700: 'team', 701: 'bear', 702: 'trap', 703: 'snowing', 704: 'smoking', 705: 'pale', 706: 'dad', 707: 'nose', 708: 'kid', 709: 'trusted', 710: 'watched', 711: 'robbed', 712: 'favor', 713: 'feeling', 714: 'undressing', 715: 'used', 716: 'cheap', 717: 'wonderful', 718: 'heart', 719: 'blew', 720: 'swimming', 721: 'paris', 722: 'side', 723: 'lit', 724: 'embarrassed', 725: 'tonight', 726: 'voice', 727: 'table', 728: 'park', 729: 'parents', 730: 'drop', 731: 'ahead', 732: 'agreed', 733: 'birds', 734: 'step', 735: 'stood', 736: 'ill', 737: 'prepared', 738: 'closer', 739: 'hated', 740: 'share', 741: 'dancing', 742: 'healthy', 743: 'talks', 744: 'woke', 745: 'cheered', 746: 'evil', 747: 'half', 748: 'jealous', 749: 'divorced', 750: 'angel', 751: 'looking', 752: 'control', 753: 'dreaming', 754: 'caught', 755: 'rope', 756: 'accept', 757: 'mom', 758: 'budge', 759: 'danger', 760: 'copy', 761: 'japan', 762: 'law', 763: 'map', 764: 'dirty', 765: 'winning', 766: 'form', 767: 'rather', 768: 'baby', 769: 'excuse', 770: 'continue', 771: 'thirty', 772: 'wanted', 773: 'surprised', 774: 'bank', 775: 'brush', 776: 'legs', 777: 'interfering', 778: 'ridiculous', 779: 'relax', 780: 'slow', 781: 'goodbye', 782: 'wet', 783: 'grab', 784: 'spoke', 785: 'runs', 786: 'fantastic', 787: 'hers', 788: 'tight', 789: 'drove', 790: 'screamed', 791: 'scared', 792: 'cheat', 793: 'glad', 794: 'shoot', 795: 'grew', 796: 'cars', 797: 'nuts', 798: 'age', 799: 'or', 800: 'mind', 801: 'innocent', 802: 'restless', 803: 'closely', 804: 'leg', 805: 'point', 806: 'ended', 807: 'dreams', 808: 'bird', 809: 'type', 810: 'american', 811: 'guilty', 812: 'finally', 813: 'soccer', 814: 'spring', 815: 'behind', 816: 'exhausted', 817: 'babbling', 818: 'ball', 819: 'knife', 820: 'genius', 821: 'glass', 822: 'laughing', 823: 'sugar', 824: 'correct', 825: 'says', 826: 'seemed', 827: 'brothers', 828: 'husband', 829: 'breathe', 830: 'writes', 831: 'matter', 832: 'cousin', 833: 'proud', 834: 'yesterday', 835: 'taught', 836: 'train', 837: 'oh', 838: 'bowed', 839: 'phoned', 840: 'refuse', 841: 'stayed', 842: 'lazy', 843: 'lies', 844: 'fainted', 845: 'then', 846: 'carry', 847: 'envy', 848: 'eaten', 849: 'sharp', 850: 'jumped', 851: 'even', 852: 'worry', 853: 'fruit', 854: 'rice', 855: 'golf', 856: 'unlucky', 857: 'escaped', 858: 'refused', 859: 'annoying', 860: 'doubts', 861: 'misled', 862: 'sue', 863: 'ears', 864: 'pregnant', 865: 'worn', 866: 'amazing', 867: 'smiling', 868: 'feet', 869: 'command', 870: 'built', 871: 'sent', 872: 'deny', 873: 'sells', 874: 'fear', 875: 'nearly', 876: 'rescued', 877: 'allow', 878: 'diet', 879: 'famous', 880: 'gets', 881: 'hoax', 882: 'unfair', 883: 'lock', 884: 'number', 885: 'stingy', 886: 'helping', 887: 'special', 888: 'kite', 889: 'sons', 890: 'sun', 891: 'teach', 892: 'heavy', 893: 'desk', 894: 'cooks', 895: 'bike', 896: 'ticket', 897: 'creep', 898: 'bite', 899: 'carefully', 900: 'honest', 901: 'death', 902: 'fever', 903: 'policeman', 904: 'defenseless', 905: 'dangerous', 906: 'radio', 907: 'sky', 908: 'cap', 909: 'singer', 910: 'hotel', 911: 'drinking', 912: 'intelligent', 913: 'pick', 914: 'housesitting', 915: 'children', 916: 'thing', 917: 'weren', 918: 'stuck', 919: 'fit', 920: 'brief', 921: 'awful', 922: 'burned', 923: 'seize', 924: 'voted', 925: 'burns', 926: 'human', 927: 'skinny', 928: 'happens', 929: 'magic', 930: 'windy', 931: 'vote', 932: 'deaf', 933: 'ugly', 934: 'wood', 935: 'decide', 936: 'lips', 937: 'loser', 938: 'rule', 939: 'thief', 940: 'huge', 941: 'crashed', 942: 'upset', 943: 'twins', 944: 'pity', 945: 'confident', 946: 'upstairs', 947: 'exist', 948: 'framed', 949: 'sushi', 950: 'games', 951: 'cab', 952: 'adult', 953: 'shopping', 954: 'burn', 955: 'shame', 956: 'focused', 957: 'boys', 958: 'answered', 959: 'canceled', 960: 'waste', 961: 'years', 962: 'rude', 963: 'tough', 964: 'body', 965: 'rush', 966: 'few', 967: 'smells', 968: 'cancer', 969: 'sports', 970: 'supper', 971: 'ride', 972: 'easygoing', 973: 'charge', 974: 'takes', 975: 'theirs', 976: 'adores', 977: 'remembers', 978: 'moment', 979: 'thinks', 980: 'sound', 981: 'cows', 982: 'grass', 983: 'towel', 984: 'year', 985: 'uncle', 986: 'animal', 987: 'disgusting', 988: 'borrow', 989: 'arrested', 990: 'sisters', 991: 'chicken', 992: 'travel', 993: 'tokyo', 994: 'having', 995: 'wide', 996: 'likely', 997: 'delicious', 998: 'their', 999: 'bill', 1000: 'ruined', 1001: 'missing', 1002: 'set', 1003: 'italy', 1004: 'silent', 1005: 'fool', 1006: 'bother', 1007: 'trip', 1008: 'camera', 1009: 'ten', 1010: 'finger', 1011: 'taxi', 1012: 'same', 1013: 'impossible', 1014: 'message', 1015: 'sour', 1016: 'office', 1017: 'seeing', 1018: 'cards', 1019: 'air', 1020: 'stole', 1021: 'cash', 1022: 'past', 1023: 'students', 1024: 'order', 1025: 'ever', 1026: 'keeps', 1027: 'diary', 1028: 'oranges', 1029: 'remained', 1030: 'terrified', 1031: 'france', 1032: 'breakfast', 1033: 'computer', 1034: 'coma', 1035: 'o', 1036: 'stomach', 1037: 'movie', 1038: 'happening', 1039: 'question', 1040: 'crime', 1041: 'makes', 1042: 'but', 1043: 'picture', 1044: 'driver', 1045: 'figure', 1046: 'pockets', 1047: 'playing', 1048: 'eye', 1049: 'decision', 1050: 'hug', 1051: 'wise', 1052: 'odd', 1053: 'ski', 1054: 'stinks', 1055: 'loosen', 1056: 'choose', 1057: 'quick', 1058: 'yell', 1059: 'hung', 1060: 'faster', 1061: 'drinks', 1062: 'drives', 1063: 'punctual', 1064: 'shout', 1065: 'coin', 1066: 'hired', 1067: 'overslept', 1068: 'sell', 1069: 'leaving', 1070: 'list', 1071: 'hugged', 1072: 'drowned', 1073: 'teaches', 1074: 'eggs', 1075: 'act', 1076: 'ship', 1077: 'egg', 1078: 'examine', 1079: 'ghosts', 1080: 'kicked', 1081: 'injured', 1082: 'china', 1083: 'pizza', 1084: 'warned', 1085: 'twice', 1086: 'lose', 1087: 'writer', 1088: 'gullible', 1089: 'relieved', 1090: 'deer', 1091: 'finished', 1092: 'panicked', 1093: 'ink', 1094: 'scare', 1095: 'cops', 1096: 'draw', 1097: 'wins', 1098: 'hole', 1099: 'eats', 1100: 'adorable', 1101: 'contributed', 1102: 'deserved', 1103: 'despise', 1104: 'ignored', 1105: 'skiing', 1106: 'trains', 1107: 'prefer', 1108: 'respect', 1109: 'slapped', 1110: 'sold', 1111: 'prove', 1112: 'expecting', 1113: 'impatient', 1114: 'returned', 1115: 'business', 1116: 'owls', 1117: 'shake', 1118: 'gentle', 1119: 'fighting', 1120: 'boat', 1121: 'girls', 1122: 'rose', 1123: 'applauded', 1124: 'meant', 1125: 'racist', 1126: 'complain', 1127: 'scolded', 1128: 'cookies', 1129: 'five', 1130: 'korean', 1131: 'wear', 1132: 'handle', 1133: 'unemployed', 1134: 'christmas', 1135: 'hour', 1136: 'risky', 1137: 'prices', 1138: 'wore', 1139: 'hopeless', 1140: 'else', 1141: 'speaking', 1142: 'police', 1143: 'meal', 1144: 'tigers', 1145: 'interesting', 1146: 'guessed', 1147: 'pencil', 1148: 'every', 1149: 'calling', 1150: 'served', 1151: 'lake', 1152: 'held', 1153: 'russian', 1154: 'piano', 1155: 'immediately', 1156: 'tank', 1157: 'costs', 1158: 'animals', 1159: 'debt', 1160: 'unconscious', 1161: 'video', 1162: 'america', 1163: 'sunny', 1164: 'complaining', 1165: 'revenge', 1166: 'studies', 1167: 'saying', 1168: 'abandoned', 1169: 'traitor', 1170: 'unreliable', 1171: 'escape', 1172: 'pleased', 1173: 'pants', 1174: 'barely', 1175: 'pray', 1176: 'seven', 1177: 'haven', 1178: 'tree', 1179: 'weight', 1180: 'knock', 1181: 'silence', 1182: 'flight', 1183: 'king', 1184: 'talented', 1185: 'position', 1186: 'astute', 1187: 'disturbing', 1188: 'facebook', 1189: 'hello', 1190: 'attack', 1191: 'resign', 1192: 'shaved', 1193: 'rained', 1194: 'course', 1195: 'waved', 1196: 'faith', 1197: 'lovely', 1198: 'admit', 1199: 'resigned', 1200: 'clumsy', 1201: 'cd', 1202: 'notes', 1203: 'alert', 1204: 'taste', 1205: 'obeyed', 1206: 'winked', 1207: 'yelled', 1208: 'cds', 1209: 'fail', 1210: 'thorough', 1211: 'contact', 1212: 'argue', 1213: 'dressed', 1214: 'eight', 1215: 'nasty', 1216: 'jerk', 1217: 'rock', 1218: 'math', 1219: 'taxes', 1220: 'dizzy', 1221: 'engaged', 1222: 'jittery', 1223: 'psyched', 1224: 'selfish', 1225: 'sloshed', 1226: 'harp', 1227: 'memorize', 1228: 'return', 1229: 'fox', 1230: 'card', 1231: 'decided', 1232: 'listens', 1233: 'relaxed', 1234: 'objective', 1235: 'anytime', 1236: 'romantic', 1237: 'chinese', 1238: 'faint', 1239: 'candy', 1240: 'chess', 1241: 'bet', 1242: 'paint', 1243: 'space', 1244: 'lion', 1245: 'drowning', 1246: 'freezing', 1247: 'stubborn', 1248: 'thinking', 1249: 'begun', 1250: 'gross', 1251: 'curse', 1252: 'goes', 1253: 'guess', 1254: 'quietly', 1255: 'pull', 1256: 'whining', 1257: 'yelling', 1258: 'switch', 1259: 'gold', 1260: 'shock', 1261: 'passed', 1262: 'reasonable', 1263: 'dishes', 1264: 'push', 1265: 'circle', 1266: 'sec', 1267: 'pinched', 1268: 'part', 1269: 'steal', 1270: 'freaked', 1271: 'winter', 1272: 'visa', 1273: 'movies', 1274: 'hiking', 1275: 'nature', 1276: 'resist', 1277: 'mouse', 1278: 'crown', 1279: 'poems', 1280: 'tourist', 1281: 'desperate', 1282: 'impressed', 1283: 'saturday', 1284: 'exciting', 1285: 'fake', 1286: 'loud', 1287: 'worth', 1288: 'paddling', 1289: 'offer', 1290: 'gambling', 1291: 'peace', 1292: 'line', 1293: 'stuff', 1294: 'adopted', 1295: 'till', 1296: 'floor', 1297: 'cousins', 1298: 'staying', 1299: 'nonsense', 1300: 'silly', 1301: 'flew', 1302: 'believed', 1303: 'brought', 1304: 'foot', 1305: 'deserve', 1306: 'stroke', 1307: 'parties', 1308: 'spiders', 1309: 'cough', 1310: 'flowers', 1311: 'sweating', 1312: 'devastated', 1313: 'fascinated', 1314: 'interested', 1315: 'making', 1316: 'holiday', 1317: 'power', 1318: 'clearly', 1319: 'fooled', 1320: 'appeared', 1321: 'obvious', 1322: 'leaks', 1323: 'dice', 1324: 'intervened', 1325: 'wind', 1326: 'ghost', 1327: 'store', 1328: 'dawn', 1329: 'helpless', 1330: 'pens', 1331: 'jobs', 1332: 'lived', 1333: 'rome', 1334: 'learning', 1335: 'ordered', 1336: 'played', 1337: 'mayor', 1338: 'bell', 1339: 'shook', 1340: 'totally', 1341: 'wouldn', 1342: 'christian', 1343: 'aware', 1344: 'color', 1345: 'handed', 1346: 'friday', 1347: 'shortcut', 1348: 'incredible', 1349: 'practicing', 1350: 'kites', 1351: 'visit', 1352: 'high', 1353: 'repeat', 1354: 'dress', 1355: 'pathetic', 1356: 'followed', 1357: 'shocked', 1358: 'teachers', 1359: 'ideas', 1360: 'available', 1361: 'zoo', 1362: 'tallest', 1363: 'apply', 1364: 'mirror', 1365: 'bugs', 1366: 'deceive', 1367: 'yourselves', 1368: 'umbrella', 1369: 'croissant', 1370: 'badly', 1371: 'feels', 1372: 'wears', 1373: 'politely', 1374: 'hardly', 1375: 'traveling', 1376: 'cream', 1377: 'regret', 1378: 'noon', 1379: 'destiny', 1380: 'probably', 1381: 'candle', 1382: 'lower', 1383: 'showed', 1384: 'clear', 1385: 'pool', 1386: 'race', 1387: 'road', 1388: 'entered', 1389: 'convinced', 1390: 'theory', 1391: 'gunfire', 1392: 'leader', 1393: 'evidence', 1394: 'gorgeous', 1395: 'under', 1396: 'hospital', 1397: 'juice', 1398: 'weapons', 1399: 'knees', 1400: 'taking', 1401: 'earring', 1402: 'photo', 1403: 'signed', 1404: 'toilet', 1405: 'using', 1406: 'neighbor', 1407: 'pure', 1408: 'goal', 1409: 'coat', 1410: 'expensive', 1411: 'golden', 1412: 'hasn', 1413: 'clothes', 1414: 'disappoint', 1415: 'invite', 1416: 'tells', 1417: 'robots', 1418: 'banged', 1419: 'taken', 1420: 'world', 1421: 'wedding', 1422: 'seats', 1423: 'comfortable', 1424: 'accepted', 1425: 'regards', 1426: 'weekend', 1427: 'awesome', 1428: 'tries', 1429: 'humor', 1430: 'thin', 1431: 'terrific', 1432: 'wept', 1433: 'aim', 1434: 'bless', 1435: 'snowed', 1436: 'above', 1437: 'cared', 1438: 'cares', 1439: 'aboard', 1440: 'coughed', 1441: 'twin', 1442: 'greedy', 1443: 'lift', 1444: 'aside', 1445: 'danced', 1446: 'fought', 1447: 'gasped', 1448: 'prayed', 1449: 'gives', 1450: 'creative', 1451: 'discreet', 1452: 'specific', 1453: 'straight', 1454: 'apologize', 1455: 'rats', 1456: 'jail', 1457: 'through', 1458: 'chilly', 1459: 'timing', 1460: 'shouted', 1461: 'attentive', 1462: 'realistic', 1463: 'boil', 1464: 'describe', 1465: 'snore', 1466: 'scream', 1467: 'dieting', 1468: 'admire', 1469: 'apologized', 1470: 'lip', 1471: 'carded', 1472: 'beans', 1473: 'honey', 1474: 'nap', 1475: 'star', 1476: 'ex', 1477: 'actor', 1478: 'doubtful', 1479: 'famished', 1480: 'rebel', 1481: 'starving', 1482: 'iron', 1483: 'walking', 1484: 'jaw', 1485: 'softly', 1486: 'arguing', 1487: 'approve', 1488: 'bores', 1489: 'cop', 1490: 'witty', 1491: 'listened', 1492: 'relented', 1493: 'pays', 1494: 'pig', 1495: 'approved', 1496: 'included', 1497: 'mistaken', 1498: 'expert', 1499: 'security', 1500: 'admired', 1501: 'coughing', 1502: 'caused', 1503: 'fed', 1504: 'kyoto', 1505: 'ring', 1506: 'volunteered', 1507: 'furious', 1508: 'artist', 1509: 'orphan', 1510: 'horrified', 1511: 'motivated', 1512: 'uninsured', 1513: 'painful', 1514: 'popular', 1515: 'latin', 1516: 'test', 1517: 'ironic', 1518: 'weapon', 1519: 'treat', 1520: 'enter', 1521: 'gate', 1522: 'neck', 1523: 'russia', 1524: 'skate', 1525: 'model', 1526: 'positive', 1527: 'shouting', 1528: 'suit', 1529: 'sweep', 1530: 'anyway', 1531: 'absurd', 1532: 'creepy', 1533: 'hesitated', 1534: 'winded', 1535: 'joined', 1536: 'trusts', 1537: 'obey', 1538: 'plans', 1539: 'ain', 1540: 'tempt', 1541: 'golfer', 1542: 'plays', 1543: 'abroad', 1544: 'depressed', 1545: 'prison', 1546: 'notice', 1547: 'homesick', 1548: 'hunch', 1549: 'ranch', 1550: 'mahjong', 1551: 'noodles', 1552: 'picnics', 1553: 'harm', 1554: 'stamp', 1555: 'punished', 1556: 'insulted', 1557: 'beginner', 1558: 'musician', 1559: 'salesman', 1560: 'astonished', 1561: 'meditating', 1562: 'optimistic', 1563: 'speechless', 1564: 'oldest', 1565: 'blunt', 1566: 'earned', 1567: 'possible', 1568: 'amusing', 1569: 'sale', 1570: 'planned', 1571: 'wednesday', 1572: 'gimmick', 1573: 'beyond', 1574: 'different', 1575: 'large', 1576: 'lend', 1577: 'sore', 1578: 'choked', 1579: 'roses', 1580: 'nurse', 1581: 'awkward', 1582: 'misses', 1583: 'evident', 1584: 'helpful', 1585: 'illegal', 1586: 'suicide', 1587: 'flag', 1588: 'roof', 1589: 'idiots', 1590: 'fact', 1591: 'blood', 1592: 'naughty', 1593: 'shallow', 1594: 'stranded', 1595: 'causes', 1596: 'wounded', 1597: 'bees', 1598: 'whistle', 1599: 'defend', 1600: 'lawn', 1601: 'painter', 1602: 'unmarried', 1603: 'gloomy', 1604: 'laughs', 1605: 'id', 1606: 'cooked', 1607: 'goats', 1608: 'terrible', 1609: 'sunburned', 1610: 'homework', 1611: 'learned', 1612: 'ladder', 1613: 'shower', 1614: 'rang', 1615: 'relied', 1616: 'osaka', 1617: 'reward', 1618: 'detective', 1619: 'owl', 1620: 'able', 1621: 'extroverted', 1622: 'brazil', 1623: 'canada', 1624: 'asking', 1625: 'vacation', 1626: 'four', 1627: 'truly', 1628: 'breathing', 1629: 'river', 1630: 'payday', 1631: 'ago', 1632: 'honor', 1633: 'fabulous', 1634: 'lead', 1635: 'smaller', 1636: 'changes', 1637: 'pass', 1638: 'drugs', 1639: 'mail', 1640: 'disappeared', 1641: 'tied', 1642: 'panting', 1643: 'nosy', 1644: 'opened', 1645: 'leaves', 1646: 'yellow', 1647: 'fate', 1648: 'spot', 1649: 'times', 1650: 'chased', 1651: 'deranged', 1652: 'handsome', 1653: 'guest', 1654: 'understands', 1655: 'newlyweds', 1656: 'separated', 1657: 'disaster', 1658: 'than', 1659: 'wings', 1660: 'barking', 1661: 'accidents', 1662: 'normally', 1663: 'rabbits', 1664: 'window', 1665: 'consult', 1666: 'cross', 1667: 'disturb', 1668: 'lecture', 1669: 'meals', 1670: 'chance', 1671: 'reason', 1672: 'without', 1673: 'college', 1674: 'gentleman', 1675: 'grown', 1676: 'violence', 1677: 'against', 1678: 'sweat', 1679: 'flu', 1680: 'australia', 1681: 'fresh', 1682: 'near', 1683: 'band', 1684: 'pointed', 1685: 'visited', 1686: 'unhappy', 1687: 'protect', 1688: 'translator', 1689: 'disappointed', 1690: 'homeschooled', 1691: 'class', 1692: 'suspect', 1693: 'scary', 1694: 'gotten', 1695: 'unlikely', 1696: 'kick', 1697: 'smokes', 1698: 'itchy', 1699: 'polite', 1700: 'stared', 1701: 'avoiding', 1702: 'touching', 1703: 'burst', 1704: 'storm', 1705: 'coach', 1706: 'deceitful', 1707: 'smiles', 1708: 'calmly', 1709: 'tortured', 1710: 'fingers', 1711: 'checks', 1712: 'match', 1713: 'hobby', 1714: 'shouldn', 1715: 'admission', 1716: 'ambulance', 1717: 'rent', 1718: 'days', 1719: 'scientist', 1720: 'skirt', 1721: 'weather', 1722: 'nightmare', 1723: 'style', 1724: 'locked', 1725: 'chair', 1726: 'minutes', 1727: 'area', 1728: 'mood', 1729: 'persuaded', 1730: 'sunday', 1731: 'greasy', 1732: 'rising', 1733: 'difficult', 1734: 'living', 1735: 'manners', 1736: 'arms', 1737: 'bothering', 1738: 'turning', 1739: 'advance', 1740: 'essential', 1741: 'low', 1742: 'dull', 1743: 'continued', 1744: 'worst', 1745: 'kidnapped', 1746: 'bucket', 1747: 'messed', 1748: 'attacked', 1749: 'speech', 1750: 'believer', 1751: 'magician', 1752: 'murderer', 1753: 'downstairs', 1754: 'hysterical', 1755: 'happiness', 1756: 'battle', 1757: 'wow', 1758: 'example', 1759: 'before', 1760: 'acted', 1761: 'exam', 1762: 'garden', 1763: 'antiques', 1764: 'expect', 1765: 'packed', 1766: 'outfit', 1767: 'daughter', 1768: 'library', 1769: 'misjudged', 1770: 'outstanding', 1771: 'neat', 1772: 'system', 1773: 'suitcase', 1774: 'bright', 1775: 'slip', 1776: 'unimportant', 1777: 'brightened', 1778: 'incompetent', 1779: 'washed', 1780: 'reasons', 1781: 'overworked', 1782: 'bunnies', 1783: 'chivalry', 1784: 'delete', 1785: 'honesty', 1786: 'cheer', 1787: 'tidy', 1788: 'marry', 1789: 'swam', 1790: 'dj', 1791: 'beg', 1792: 'tripped', 1793: 'dozed', 1794: 'higher', 1795: 'chuckled', 1796: 'disagree', 1797: 'art', 1798: 'threw', 1799: 'buying', 1800: 'twelve', 1801: 'jesus', 1802: 'chat', 1803: 'comment', 1804: 'agrees', 1805: 'cheats', 1806: 'sensible', 1807: 'tolerant', 1808: 'comfort', 1809: 'flip', 1810: 'guts', 1811: 'dozing', 1812: 'taller', 1813: 'sand', 1814: 'jazz', 1815: 'ufo', 1816: 'finicky', 1817: 'starved', 1818: 'cost', 1819: 'yen', 1820: 'cloudy', 1821: 'means', 1822: 'blushed', 1823: 'clapped', 1824: 'giggled', 1825: 'prepaid', 1826: 'dope', 1827: 'beef', 1828: 'forward', 1829: 'safely', 1830: 'morning', 1831: 'easter', 1832: 'donut', 1833: 'snack', 1834: 'courage', 1835: 'avoids', 1836: 'denied', 1837: 'cranky', 1838: 'arrogant', 1839: 'horrible', 1840: 'praying', 1841: 'detest', 1842: 'woozy', 1843: 'liars', 1844: 'opera', 1845: 'trips', 1846: 'wonder', 1847: 'risk', 1848: 'anorexic', 1849: 'autistic', 1850: 'barefoot', 1851: 'grateful', 1852: 'hesitant', 1853: 'solid', 1854: 'wolf', 1855: 'trick', 1856: 'bedtime', 1857: 'digging', 1858: 'driving', 1859: 'pigs', 1860: 'recess', 1861: 'sued', 1862: 'louder', 1863: 'pushing', 1864: 'vet', 1865: 'loyal', 1866: 'picky', 1867: 'shrugged', 1868: 'stutters', 1869: 'vanished', 1870: 'stoned', 1871: 'unbelievable', 1872: 'succeeded', 1873: 'surrender', 1874: 'adults', 1875: 'pooped', 1876: 'respectful', 1877: 'supportive', 1878: 'beware', 1879: 'feed', 1880: 'fry', 1881: 'ham', 1882: 'dug', 1883: 'skating', 1884: 'sexist', 1885: 'demented', 1886: 'calls', 1887: 'honk', 1888: 'horn', 1889: 'outraged', 1890: 'sneezing', 1891: 'belong', 1892: 'blacked', 1893: 'designed', 1894: 'bmw', 1895: 'exaggerated', 1896: 'flying', 1897: 'bomb', 1898: 'orders', 1899: 'fall', 1900: 'camels', 1901: 'sweets', 1902: 'tulips', 1903: 'arabic', 1904: 'baking', 1905: 'crew', 1906: 'outrank', 1907: 'violin', 1908: 'motel', 1909: 'plane', 1910: 'queen', 1911: 'unarmed', 1912: 'witness', 1913: 'addict', 1914: 'impartial', 1915: 'intrigued', 1916: 'bossy', 1917: 'strike', 1918: 'sensitive', 1919: 'private', 1920: 'legal', 1921: 'gag', 1922: 'apart', 1923: 'official', 1924: 'italian', 1925: 'aches', 1926: 'robe', 1927: 'beside', 1928: 'stir', 1929: 'freaky', 1930: 'embraced', 1931: 'spies', 1932: 'graduated', 1933: 'cow', 1934: 'hoot', 1935: 'wimp', 1936: 'absent', 1937: 'fierce', 1938: 'rested', 1939: 'wicked', 1940: 'scares', 1941: 'staggered', 1942: 'harsh', 1943: 'snoring', 1944: 'wax', 1945: 'doomed', 1946: 'survive', 1947: 'lawyers', 1948: 'sinking', 1949: 'relief', 1950: 'hypocrite', 1951: 'coke', 1952: 'lay', 1953: 'shovel', 1954: 'promise', 1955: 'van', 1956: 'hint', 1957: 'mug', 1958: 'holidays', 1959: 'accelerated', 1960: 'deceived', 1961: 'beard', 1962: 'respects', 1963: 'slacker', 1964: 'dislike', 1965: 'honored', 1966: 'insects', 1967: 'ironing', 1968: 'spinach', 1969: 'seafood', 1970: 'upstate', 1971: 'harvard', 1972: 'sunsets', 1973: 'answers', 1974: 'plead', 1975: 'puppy', 1976: 'direct', 1977: 'scold', 1978: 'diabetic', 1979: 'prisoner', 1980: 'diplomatic', 1981: 'remodeling', 1982: 'classic', 1983: 'identical', 1984: 'ache', 1985: 'itches', 1986: 'hatch', 1987: 'pack', 1988: 'bags', 1989: 'prepare', 1990: 'dumped', 1991: 'sings', 1992: 'teased', 1993: 'beauty', 1994: 'gossiping', 1995: 'grumbling', 1996: 'farce', 1997: 'hogwash', 1998: 'immoral', 1999: 'obscene', 2000: 'lame', 2001: 'melted', 2002: 'struggled', 2003: 'bites', 2004: 'improvised', 2005: 'ashamed', 2006: 'devious', 2007: 'remembered', 2008: 'tears', 2009: 'understood', 2010: 'humble', 2011: 'charming', 2012: 'doctors', 2013: 'country', 2014: 'letdown', 2015: 'thought', 2016: 'tragedy', 2017: 'painted', 2018: 'fraud', 2019: 'capsized', 2020: 'present', 2021: 'round', 2022: 'interfere', 2023: 'remind', 2024: 'pills', 2025: 'cane', 2026: 'suits', 2027: 'jeans', 2028: 'ethiopian', 2029: 'dreamer', 2030: 'cheating', 2031: 'standing', 2032: 'nara', 2033: 'comedian', 2034: 'gardener', 2035: 'humiliating', 2036: 'tongue', 2037: 'afford', 2038: 'football', 2039: 'goatee', 2040: 'ankle', 2041: 'elbow', 2042: 'cartoons', 2043: 'eggplant', 2044: 'weddings', 2045: 'outwitted', 2046: 'refund', 2047: 'mommy', 2048: 'concerned', 2049: 'dismissed', 2050: 'skeptical', 2051: 'fork', 2052: 'arrange', 2053: 'assist', 2054: 'ford', 2055: 'notify', 2056: 'carpenter', 2057: 'lifeguard', 2058: 'engineer', 2059: 'begging', 2060: 'pickle', 2061: 'minded', 2062: 'responsible', 2063: 'modest', 2064: 'city', 2065: 'surprise', 2066: 'irrelevant', 2067: 'sweltering', 2068: 'wrap', 2069: 'lemons', 2070: 'glowing', 2071: 'niece', 2072: 'bottle', 2073: 'remove', 2074: 'airmail', 2075: 'forgave', 2076: 'stabbed', 2077: 'alarm', 2078: 'squabbling', 2079: 'aspirin', 2080: 'net', 2081: 'soldier', 2082: 'feared', 2083: 'greeted', 2084: 'interns', 2085: 'gifts', 2086: 'each', 2087: 'believes', 2088: 'legend', 2089: 'bleeding', 2090: 'involved', 2091: 'likeable', 2092: 'training', 2093: 'jokes', 2094: 'texted', 2095: 'conscious', 2096: 'lack', 2097: 'waffles', 2098: 'ought', 2099: 'canadians', 2100: 'stolen', 2101: 'stake', 2102: 'prize', 2103: 'comb', 2104: 'decisive', 2105: 'stalling', 2106: 'hurting', 2107: 'flexible', 2108: 'inbox', 2109: 'potatoes', 2110: 'foolish', 2111: 'glum', 2112: 'faces', 2113: 'purpose', 2114: 'hammer', 2115: 'stairs', 2116: 'eggnog', 2117: 'joy', 2118: 'recently', 2119: 'fixed', 2120: 'gray', 2121: 'picasso', 2122: 'carrots', 2123: 'pajamas', 2124: 'stepdad', 2125: 'overweight', 2126: 'hunt', 2127: 'ipad', 2128: 'stark', 2129: 'health', 2130: 'abhor', 2131: 'foreigner', 2132: 'professor', 2133: 'dumbfounded', 2134: 'appreciate', 2135: 'reach', 2136: 'powerless', 2137: 'hypocrisy', 2138: 'surprises', 2139: 'beach', 2140: 'moscow', 2141: 'turkey', 2142: 'miracle', 2143: 'aid', 2144: 'soap', 2145: 'outsmarted', 2146: 'flute', 2147: 'mile', 2148: 'label', 2149: 'stifled', 2150: 'grin', 2151: 'divorce', 2152: 'dumbstruck', 2153: 'support', 2154: 'citizen', 2155: 'suicidal', 2156: 'reachable', 2157: 'contagious', 2158: 'suitable', 2159: 'flower', 2160: 'hours', 2161: 'nine', 2162: 'sofa', 2163: 'asia', 2164: 'informed', 2165: 'repair', 2166: 'tiny', 2167: 'pulse', 2168: 'scarf', 2169: 'beats', 2170: 'windows', 2171: 'ketchup', 2172: 'allowed', 2173: 'weep', 2174: 'despised', 2175: 'despises', 2176: 'reply', 2177: 'rejected', 2178: 'promoted', 2179: 'worships', 2180: 'apologizing', 2181: 'precautions', 2182: 'dishonest', 2183: 'offensive', 2184: 'ballon', 2185: 'curtain', 2186: 'milkman', 2187: 'daises', 2188: 'demoted', 2189: 'thanked', 2190: 'immature', 2191: 'luxury', 2192: 'relaxing', 2193: 'tastes', 2194: 'loses', 2195: 'belongs', 2196: 'fighter', 2197: 'schemer', 2198: 'nephew', 2199: 'weakening', 2200: 'oven', 2201: 'punched', 2202: 'wandered', 2203: 'watches', 2204: 'depend', 2205: 'evacuate', 2206: 'withdraw', 2207: 'sophomores', 2208: 'surrounded', 2209: 'murdered', 2210: 'younger', 2211: 'necessary', 2212: 'add', 2213: 'adjust', 2214: 'brakes', 2215: 'equal', 2216: 'arrest', 2217: 'error', 2218: 'others', 2219: 'behave', 2220: 'deeply', 2221: 'rely', 2222: 'click', 2223: 'link', 2224: 'pet', 2225: 'proof', 2226: 'coward', 2227: 'exercise', 2228: 'goodnight', 2229: 'headache', 2230: 'biologist', 2231: 'forty', 2232: 'influential', 2233: 'owns', 2234: 'land', 2235: 'pushed', 2236: 'sailor', 2237: 'studied', 2238: 'twisted', 2239: 'scripts', 2240: 'overreacting', 2241: 'address', 2242: 'vegetarian', 2243: 'cactus', 2244: 'backache', 2245: 'hangover', 2246: 'otherwise', 2247: 'guitar', 2248: 'apology', 2249: 'freedom', 2250: 'jacket', 2251: 'lottery', 2252: 'london', 2253: 'giving', 2254: 'forgiven', 2255: 'diarrhea', 2256: 'egyptian', 2257: 'familiar', 2258: 'nonstop', 2259: 'grow', 2260: 'menu', 2261: 'downtown', 2262: 'hustle', 2263: 'toss', 2264: 'price', 2265: 'aching', 2266: 'plug', 2267: 'screw', 2268: 'aggressive', 2269: 'secrets', 2270: 'bitterly', 2271: 'hyperactive', 2272: 'solve', 2273: 'bar', 2274: 'tasty', 2275: 'market', 2276: 'gods', 2277: 'gibberish', 2278: 'hilarious', 2279: 'skin', 2280: 'indiscreet', 2281: 'lacks', 2282: 'cigar', 2283: 'stove', 2284: 'fare', 2285: 'recognized', 2286: 'nda', 2287: 'threatened', 2288: 'unstoppable', 2289: 'weeks', 2290: 'neighbors', 2291: 'adventurous', 2292: 'arrive', 2293: 'actresses', 2294: 'faced', 2295: 'european', 2296: 'buddhist', 2297: 'blaming', 2298: 'changing', 2299: 'divide', 2300: 'bleed', 2301: 'rid', 2302: 'remote', 2303: 'foolishly', 2304: 'knee', 2305: 'enjoyed', 2306: 'forced', 2307: 'adventure', 2308: 'pulled', 2309: 'raised', 2310: 'rarely', 2311: 'fluently', 2312: 'englishman', 2313: 'aristocrat', 2314: 'completely', 2315: 'vodka', 2316: 'dreamt', 2317: 'goat', 2318: 'flat', 2319: 'tire', 2320: 'information', 2321: 'patience', 2322: 'ends', 2323: 'candlelight', 2324: 'custom', 2325: 'helsinki', 2326: 'graduate', 2327: 'month', 2328: 'destroy', 2329: 'remain', 2330: 'attorney', 2331: 'hire', 2332: 'television', 2333: 'betray', 2334: 'swimmer', 2335: 'kitchen', 2336: 'chained', 2337: 'loaded', 2338: 'design', 2339: 'flaw', 2340: 'europe', 2341: 'greece', 2342: 'grapes', 2343: 'project', 2344: 'strength', 2345: 'begins', 2346: 'schools', 2347: 'picked', 2348: 'countdown', 2349: 'tickles', 2350: 'names', 2351: 'fascinating', 2352: 'lights', 2353: 'parrot', 2354: 'soldiers', 2355: 'shining', 2356: 'words', 2357: 'families', 2358: 'christians', 2359: 'bizarre', 2360: 'top', 2361: 'shoelaces', 2362: 'injury', 2363: 'onto', 2364: 'patronizing', 2365: 'risks', 2366: 'respected', 2367: 'trunk', 2368: 'frightened', 2369: 'indecisive', 2370: 'potato', 2371: 'contest', 2372: 'ambidextrous', 2373: 'ignoring', 2374: 'agreement', 2375: 'fellow', 2376: 'experience', 2377: 'motivates', 2378: 'wearing', 2379: 'oath', 2380: 'cowards', 2381: 'negative', 2382: 'soaking', 2383: 'whale', 2384: 'mammal', 2385: 'credit', 2386: 'verify', 2387: 'climb', 2388: 'tip', 2389: 'gum', 2390: 'bowling', 2391: 'vegetables', 2392: 'cigarette', 2393: 'papers', 2394: 'climbed', 2395: 'overconfident', 2396: 'shop', 2397: 'tetris', 2398: 'novel', 2399: 'virtue', 2400: 'terms', 2401: 'anywhere', 2402: 'peel', 2403: 'hop', 2404: 'skip', 2405: 'cuff', 2406: 'bark', 2407: 'cringed', 2408: 'pro', 2409: 'obese', 2410: 'pardon', 2411: 'search', 2412: 'seriously', 2413: 'knits', 2414: 'rocks', 2415: 'swims', 2416: 'swore', 2417: 'content', 2418: 'exists', 2419: 'tragic', 2420: 'bushed', 2421: 'immune', 2422: 'lasts', 2423: 'mama', 2424: 'replace', 2425: 'flies', 2426: 'burped', 2427: 'dances', 2428: 'goofed', 2429: 'moaned', 2430: 'nodded', 2431: 'paused', 2432: 'sighed', 2433: 'snores', 2434: 'yawned', 2435: 'merciful', 2436: 'vigilant', 2437: 'watchful', 2438: 'bowl', 2439: 'faking', 2440: 'monk', 2441: 'confessed', 2442: 'baker', 2443: 'fasting', 2444: 'neutral', 2445: 'resting', 2446: 'vice', 2447: 'futile', 2448: 'release', 2449: 'action', 2450: 'exhaled', 2451: 'gloated', 2452: 'grinned', 2453: 'groaned', 2454: 'grunted', 2455: 'ocd', 2456: 'inhaled', 2457: 'kneeled', 2458: 'slipped', 2459: 'sneezed', 2460: 'flaky', 2461: 'heel', 2462: 'loss', 2463: 'ego', 2464: 'drew', 2465: 'abandon', 2466: 'merciless', 2467: 'backup', 2468: 'fbi', 2469: 'ramble', 2470: 'driven', 2471: 'bigot', 2472: 'ninja', 2473: 'caviar', 2474: 'mugged', 2475: 'sympathize', 2476: 'farmer', 2477: 'surfer', 2478: 'waiter', 2479: 'agent', 2480: 'educated', 2481: 'grieving', 2482: 'perth', 2483: 'literate', 2484: 'painting', 2485: 'rational', 2486: 'thirteen', 2487: 'cured', 2488: 'lucid', 2489: 'elk', 2490: 'hip', 2491: 'gawking', 2492: 'staring', 2493: 'sweated', 2494: 'approves', 2495: 'enlisted', 2496: 'flinched', 2497: 'insisted', 2498: 'timid', 2499: 'groggy', 2500: 'focus', 2501: 'goners', 2502: 'feast', 2503: 'phony', 2504: 'paying', 2505: 'amuse', 2506: 'cain', 2507: 'despair', 2508: 'mock', 2509: 'sass', 2510: 'drain', 2511: 'oil', 2512: 'grows', 2513: 'blog', 2514: 'maid', 2515: 'tycoon', 2516: 'saint', 2517: 'bled', 2518: 'thrilling', 2519: 'acquired', 2520: 'runner', 2521: 'borrowed', 2522: 'bribed', 2523: 'poker', 2524: 'queasy', 2525: 'reborn', 2526: 'bonus', 2527: 'date', 2528: 'braces', 2529: 'rights', 2530: 'sinned', 2531: 'tenure', 2532: 'sirens', 2533: 'voices', 2534: 'clocks', 2535: 'sewing', 2536: 'spoons', 2537: 'garlic', 2538: 'object', 2539: 'lamp', 2540: 'loan', 2541: 'yacht', 2542: 'squash', 2543: 'rescheduled', 2544: 'resent', 2545: 'bacon', 2546: 'smelled', 2547: 'firefox', 2548: 'icelandic', 2549: 'gang', 2550: 'spy', 2551: 'wasted', 2552: 'bat', 2553: 'wig', 2554: 'sickens', 2555: 'gift', 2556: 'p', 2557: 'sequel', 2558: 'deserted', 2559: 'improved', 2560: 'obsolete', 2561: 'occupied', 2562: 'outdated', 2563: 'rat', 2564: 'proceed', 2565: 'mortal', 2566: 'scarce', 2567: 'lungs', 2568: 'hood', 2569: 'bent', 2570: 'obeys', 2571: 'meddling', 2572: 'shooting', 2573: 'worrying', 2574: 'saturn', 2575: 'myth', 2576: 'unreal', 2577: 'vulgar', 2578: 'asian', 2579: 'armed', 2580: 'pun', 2581: 'exercised', 2582: 'exercises', 2583: 'strict', 2584: 'recovered', 2585: 'aloof', 2586: 'klutz', 2587: 'fiasco', 2588: 'ails', 2589: 'disagreed', 2590: 'biased', 2591: 'qualified', 2592: 'autumn', 2593: 'brace', 2594: 'file', 2595: 'recycle', 2596: 'annoy', 2597: 'struggle', 2598: 'duck', 2599: 'cries', 2600: 'lines', 2601: 'afternoon', 2602: 'dislikes', 2603: 'tan', 2604: 'bankrupt', 2605: 'homeless', 2606: 'outgoing', 2607: 'learns', 2608: 'jelly', 2609: 'florist', 2610: 'author', 2611: 'ached', 2612: 'czech', 2613: 'nail', 2614: 'ease', 2615: 'guarantee', 2616: 'mondays', 2617: 'sundays', 2618: 'karaoke', 2619: 'riddles', 2620: 'hiccup', 2621: 'hurried', 2622: 'castles', 2623: 'history', 2624: 'jogging', 2625: 'puzzles', 2626: 'sashimi', 2627: 'stories', 2628: 'turtles', 2629: 'lasagna', 2630: 'medic', 2631: 'ruler', 2632: 'asap', 2633: 'surgery', 2634: 'cattle', 2635: 'sneeze', 2636: 'fan', 2637: 'captured', 2638: 'welcomed', 2639: 'prosper', 2640: 'extra', 2641: 'atheist', 2642: 'dependable', 2643: 'quitter', 2644: 'stew', 2645: 'insult', 2646: 'redundant', 2647: 'vibrating', 2648: 'searching', 2649: 'lean', 2650: 'dutch', 2651: 'tag', 2652: 'practice', 2653: 'lynn', 2654: 'larger', 2655: 'joints', 2656: 'tummy', 2657: 'meter', 2658: 'types', 2659: 'counting', 2660: 'resisting', 2661: 'screaming', 2662: 'cyanide', 2663: 'plastic', 2664: 'barked', 2665: 'siren', 2666: 'quarreled', 2667: 'starve', 2668: 'hollow', 2669: 'scam', 2670: 'approached', 2671: 'asthma', 2672: 'dwarf', 2673: 'pilot', 2674: 'miserly', 2675: 'idol', 2676: 'shaking', 2677: 'spoiled', 2678: 'paged', 2679: 'shed', 2680: 'sauce', 2681: 'surrendered', 2682: 'credible', 2683: 'concept', 2684: 'wipe', 2685: 'morons', 2686: 'disgust', 2687: 'freak', 2688: 'prude', 2689: 'instead', 2690: 'balls', 2691: 'precise', 2692: 'bullet', 2693: 'mice', 2694: 'boxes', 2695: 'congratulations', 2696: 'dig', 2697: 'rap', 2698: 'interrupt', 2699: 'overdo', 2700: 'whirl', 2701: 'popcorn', 2702: 'bottom', 2703: 'toyota', 2704: 'racket', 2705: 'mark', 2706: 'disney', 2707: 'robot', 2708: 'mentioned', 2709: 'pressured', 2710: 'whisky', 2711: 'tends', 2712: 'freshman', 2713: 'unfortunate', 2714: 'spain', 2715: 'booked', 2716: 'undo', 2717: 'chickened', 2718: 'feverish', 2719: 'politics', 2720: 'raccoons', 2721: 'reptiles', 2722: 'tomatoes', 2723: 'dandruff', 2724: 'drum', 2725: 'rains', 2726: 'imagined', 2727: 'climbing', 2728: 'teaching', 2729: 'heat', 2730: 'hyogo', 2731: 'milan', 2732: 'interest', 2733: 'comedies', 2734: 'internet', 2735: 'tissue', 2736: 'scissors', 2737: 'shave', 2738: 'bills', 2739: 'baseball', 2740: 'bass', 2741: 'biking', 2742: 'recognize', 2743: 'spanish', 2744: 'dollar', 2745: 'disgusted', 2746: 'petrified', 2747: 'bbc', 2748: 'optimist', 2749: 'catching', 2750: 'cracking', 2751: 'zambia', 2752: 'portugal', 2753: 'nearsighted', 2754: 'alarmed', 2755: 'certain', 2756: 'wealthy', 2757: 'malaysian', 2758: 'poisonous', 2759: 'llama', 2760: 'option', 2761: 'tasted', 2762: 'thrill', 2763: 'ages', 2764: 'soft', 2765: 'eyesore', 2766: 'artificial', 2767: 'dinnertime', 2768: 'dirt', 2769: 'inevitable', 2770: 'misleading', 2771: 'refreshing', 2772: 'koalas', 2773: 'kumi', 2774: 'metal', 2775: 'celebrate', 2776: 'widow', 2777: 'merry', 2778: 'shift', 2779: 'squeak', 2780: 'bore', 2781: 'saddle', 2782: 'carefree', 2783: 'graceful', 2784: 'brazilian', 2785: 'assertive', 2786: 'whimpering', 2787: 'church', 2788: 'pagoda', 2789: 'childish', 2790: 'personal', 2791: 'snag', 2792: 'damp', 2793: 'cast', 2794: 'growled', 2795: 'engine', 2796: 'moon', 2797: 'radar', 2798: 'sea', 2799: 'cure', 2800: 'salt', 2801: 'actors', 2802: 'melons', 2803: 'pilots', 2804: 'respond', 2805: 'garbage', 2806: 'pleases', 2807: 'limited', 2808: 'err', 2809: 'baked', 2810: 'blamed', 2811: 'disgusts', 2812: 'exaggerates', 2813: 'hiccups', 2814: 'muslim', 2815: 'banker', 2816: 'cyborg', 2817: 'maniac', 2818: 'priest', 2819: 'friendly', 2820: 'court', 2821: 'insecure', 2822: 'insolent', 2823: 'inspired', 2824: 'buddy', 2825: 'perverse', 2826: 'perky', 2827: 'tense', 2828: 'weary', 2829: 'nudged', 2830: 'pitied', 2831: 'sees', 2832: 'yawn', 2833: 'unlock', 2834: 'toes', 2835: 'raw', 2836: 'shared', 2837: 'justice', 2838: 'gifu', 2839: 'organized', 2840: 'prisoners', 2841: 'its', 2842: 'dude', 2843: 'received', 2844: 'unsure', 2845: 'devil', 2846: 'mustn', 2847: 'reliable', 2848: 'less', 2849: 'invisible', 2850: 'wizard', 2851: 'blushing', 2852: 'atlantis', 2853: 'frank', 2854: 'bears', 2855: 'sometime', 2856: 'excused', 2857: 'cherries', 2858: 'blinds', 2859: 'drawer', 2860: 'concentrate', 2861: 'cookie', 2862: 'supply', 2863: 'bridge', 2864: 'street', 2865: 'brute', 2866: 'exaggerate', 2867: 'mention', 2868: 'mislead', 2869: 'oppose', 2870: 'rip', 2871: 'sword', 2872: 'fetch', 2873: 'garage', 2874: 'behaved', 2875: 'deserves', 2876: 'backward', 2877: 'distracted', 2878: 'hunting', 2879: 'moves', 2880: 'screams', 2881: 'shares', 2882: 'tenacious', 2883: 'crude', 2884: 'historian', 2885: 'lisbon', 2886: 'senile', 2887: 'wealth', 2888: 'album', 2889: 'embarrassing', 2890: 'melodramatic', 2891: 'masochist', 2892: 'norway', 2893: 'frying', 2894: 'witch', 2895: 'bake', 2896: 'minds', 2897: 'flattered', 2898: 'refreshed', 2899: 'chemistry', 2900: 'grenade', 2901: 'scooter', 2902: 'heartburn', 2903: 'future', 2904: 'chocolate', 2905: 'languages', 2906: 'pop', 2907: 'tahiti', 2908: 'astronomy', 2909: 'fortune', 2910: 'massage', 2911: 'cello', 2912: 'recommend', 2913: 'maui', 2914: 'require', 2915: 'umbrellas', 2916: 'martini', 2917: 'moody', 2918: 'checking', 2919: 'freaking', 2920: 'romania', 2921: 'inviting', 2922: 'middle', 2923: 'catholic', 2924: 'beggar', 2925: 'snitch', 2926: 'bluffing', 2927: 'wasting', 2928: 'saving', 2929: 'drawn', 2930: 'puzzle', 2931: 'dvd', 2932: 'euros', 2933: 'creeps', 2934: 'freaks', 2935: 'ambush', 2936: 'drizzling', 2937: 'excessive', 2938: 'greek', 2939: 'group', 2940: 'rainy', 2941: 'complicated', 2942: 'season', 2943: 'drug', 2944: 'monkey', 2945: 'logical', 2946: 'natural', 2947: 'bitter', 2948: 'sunburn', 2949: 'throat', 2950: 'toy', 2951: 'responded', 2952: 'gain', 2953: 'pets', 2954: 'trigger', 2955: 'article', 2956: 'prayers', 2957: 'defeated', 2958: 'idolized', 2959: 'pianist', 2960: 'shameless', 2961: 'socks', 2962: 'repulses', 2963: 'rode', 2964: 'camel', 2965: 'happily', 2966: 'sadly', 2967: 'startled', 2968: 'size', 2969: 'nosey', 2970: 'pinching', 2971: 'scaring', 2972: 'showing', 2973: 'kindly', 2974: 'concerns', 2975: 'enormous', 2976: 'confusing', 2977: 'debatable', 2978: 'idle', 2979: 'affair', 2980: 'troubling', 2981: 'cage', 2982: 'ticked', 2983: 'creaked', 2984: 'jury', 2985: 'mic', 2986: 'icy', 2987: 'abated', 2988: 'wound', 2989: 'healed', 2990: 'limit', 2991: 'singers', 2992: 'grave', 2993: 'nearby', 2994: 'traitors', 2995: 'sabotage', 2996: 'norm', 2997: 'admires', 2998: 'avoided', 2999: 'babysat', 3000: 'detests', 3001: 'drown', 3002: 'tb', 3003: 'frightens', 3004: 'alibi', 3005: 'freckles', 3006: 'insomnia', 3007: 'olives', 3008: 'fascist', 3009: 'patriot', 3010: 'plumber', 3011: 'recluse', 3012: 'redneck', 3013: 'refugee', 3014: 'surgeon', 3015: 'trucker', 3016: 'veteran', 3017: 'widower', 3018: 'ambitious', 3019: 'brilliant', 3020: 'eccentric', 3021: 'emotional', 3022: 'energetic', 3023: 'improving', 3024: 'insincere', 3025: 'muttering', 3026: 'obsessive', 3027: 'perplexed', 3028: 'undecided', 3029: 'whistling', 3030: 'hockey', 3031: 'ponies', 3032: 'reggae', 3033: 'shouts', 3034: 'reads', 3035: 'log', 3036: 'slugged', 3037: 'pony', 3038: 'executed', 3039: 'perk', 3040: 'succeed', 3041: 'award', 3042: 'slower', 3043: 'unfasten', 3044: 'pin', 3045: 'plants', 3046: 'adore', 3047: 'swordfish', 3048: 'competed', 3049: 'disturbed', 3050: 'crisis', 3051: 'visitors', 3052: 'classmates', 3053: 'historians', 3054: 'lifeguards', 3055: 'professors', 3056: 'plenty', 3057: 'view', 3058: 'rotten', 3059: 'cause', 3060: 'whatever', 3061: 'customs', 3062: 'razor', 3063: 'exit', 3064: 'wherever', 3065: 'hamlet', 3066: 'conform', 3067: 'conceited', 3068: 'practical', 3069: 'owner', 3070: 'mauled', 3071: 'candles', 3072: 'religious', 3073: 'build', 3074: 'nests', 3075: 'bow', 3076: 'champagne', 3077: 'insist', 3078: 'imminent', 3079: 'dial', 3080: 'ants', 3081: 'wanna', 3082: 'threaten', 3083: 'outdoors', 3084: 'spoon', 3085: 'ladies', 3086: 'haste', 3087: 'hay', 3088: 'blackmailed', 3089: 'blocked', 3090: 'hanged', 3091: 'remorse', 3092: 'houses', 3093: 'poet', 3094: 'daredevil', 3095: 'introverted', 3096: 'smoker', 3097: 'struck', 3098: 'vitamins', 3099: 'violated', 3100: 'wakes', 3101: 'imprisoned', 3102: 'raging', 3103: 'timer', 3104: 'receipt', 3105: 'elevator', 3106: 'handrail', 3107: 'complex', 3108: 'cakes', 3109: 'ecuador', 3110: 'shikoku', 3111: 'baited', 3112: 'hook', 3113: 'hybrid', 3114: 'upfront', 3115: 'chopin', 3116: 'consented', 3117: 'demand', 3118: 'slap', 3119: 'bee', 3120: 'sting', 3121: 'lightly', 3122: 'accident', 3123: 'hypocrites', 3124: 'mosquitoes', 3125: 'desert', 3126: 'migraine', 3127: 'earache', 3128: 'discs', 3129: 'nieces', 3130: 'folk', 3131: 'songs', 3132: 'sandwiches', 3133: 'sauerkraut', 3134: 'watermelon', 3135: 'film', 3136: 'california', 3137: 'fuel', 3138: 'curb', 3139: 'airplane', 3140: 'sunrise', 3141: 'corrected', 3142: 'usually', 3143: 'discount', 3144: 'compete', 3145: 'confess', 3146: 'discouraged', 3147: 'judge', 3148: 'salesperson', 3149: 'astronomer', 3150: 'bringing', 3151: 'concentrating', 3152: 'pear', 3153: 'fairly', 3154: 'keeping', 3155: 'eighteen', 3156: 'double', 3157: 'twitter', 3158: 'client', 3159: 'rumors', 3160: 'identify', 3161: 'curfew', 3162: 'stunning', 3163: 'fatal', 3164: 'pieces', 3165: 'ways', 3166: 'heavily', 3167: 'flimsy', 3168: 'cargo', 3169: 'compliment', 3170: 'conspiracy', 3171: 'dictionary', 3172: 'emergency', 3173: 'confidential', 3174: 'exhilarating', 3175: 'nicely', 3176: 'pocket', 3177: 'questionable', 3178: 'chin', 3179: 'investigate', 3180: 'enjoyable', 3181: 'conquers', 3182: 'prediction', 3183: 'numb', 3184: 'shoulder', 3185: 'volunteers', 3186: 'attention', 3187: 'reconsider', 3188: 'leak', 3189: 'poets', 3190: 'pretend', 3191: 'rugs', 3192: 'absorb', 3193: 'german', 3194: 'polish', 3195: 'loudly', 3196: 'rapidly', 3197: 'fashionable', 3198: 'spin', 3199: 'webs', 3200: 'stealing', 3201: 'medicine', 3202: 'termites', 3203: 'knot', 3204: 'loose', 3205: 'fir', 3206: 'blow', 3207: 'pheasant', 3208: 'terrifying', 3209: 'spirit', 3210: 'handy', 3211: 'squeaked', 3212: 'doorbell', 3213: 'earth', 3214: 'rotates', 3215: 'lid', 3216: 'lovers', 3217: 'sail', 3218: 'thick', 3219: 'tactic', 3220: 'hitch', 3221: 'trial', 3222: 'carnage', 3223: 'genuine', 3224: 'hunted', 3225: 'foxes', 3226: 'extortion', 3227: 'spaghetti', 3228: 'fishy', 3229: 'thursday', 3230: 'arrives', 3231: 'appear', 3232: 'disliked', 3233: 'dyed', 3234: 'gazed', 3235: 'allergies', 3236: 'iphone', 3237: 'arthritis', 3238: 'income', 3239: 'wavy', 3240: 'imitated', 3241: 'civilian', 3242: 'democrat', 3243: 'designer', 3244: 'lobbyist', 3245: 'mechanic', 3246: 'pacifist', 3247: 'reporter', 3248: 'sorcerer', 3249: 'teenager', 3250: 'weakling', 3251: 'amateur', 3252: 'growing', 3253: 'intolerant', 3254: 'mesmerized', 3255: 'fiance', 3256: 'packing', 3257: 'passionate', 3258: 'stuttering', 3259: 'winner', 3260: 'unfaithful', 3261: 'vulnerable', 3262: 'crook', 3263: 'budging', 3264: 'violent', 3265: 'empathy', 3266: 'blondes', 3267: 'lobster', 3268: 'knit', 3269: 'misunderstood', 3270: 'offended', 3271: 'outlived', 3272: 'owned', 3273: 'poisoned', 3274: 'released', 3275: 'retired', 3276: 'patiently', 3277: 'skipped', 3278: 'swindled', 3279: 'vomited', 3280: 'results', 3281: 'acquitted', 3282: 'strangled', 3283: 'poetry', 3284: 'beers', 3285: 'liquid', 3286: 'flaws', 3287: 'australian', 3288: 'cultivate', 3289: 'visitor', 3290: 'mysteries', 3291: 'rented', 3292: 'canoe', 3293: 'seek', 3294: 'guinea', 3295: 'journalists', 3296: 'workaholics', 3297: 'collection', 3298: 'invented', 3299: 'suffer', 3300: 'flammable', 3301: 'stressed', 3302: 'blowing', 3303: 'scratched', 3304: 'guard', 3305: 'productive', 3306: 'anxious', 3307: 'bananas', 3308: 'bookmark', 3309: 'site', 3310: 'chen', 3311: 'consider', 3312: 'facts', 3313: 'bribe', 3314: 'conquer', 3315: 'octopus', 3316: 'kabuki', 3317: 'trim', 3318: 'gums', 3319: 'offend', 3320: 'paranoid', 3321: 'hog', 3322: 'overthink', 3323: 'patronize', 3324: 'chances', 3325: 'stones', 3326: 'relaxes', 3327: 'matters', 3328: 'blanks', 3329: 'broom', 3330: 'wrench', 3331: 'thanksgiving', 3332: 'abused', 3333: 'cycling', 3334: 'gripped', 3335: 'footsteps', 3336: 'mere', 3337: 'classmate', 3338: 'colleague', 3339: 'memory', 3340: 'gossip', 3341: 'rolled', 3342: 'precedent', 3343: 'wallet', 3344: 'touched', 3345: 'appeal', 3346: 'heartbroken', 3347: 'bodybuilder', 3348: 'creationist', 3349: 'keeper', 3350: 'stepfather', 3351: 'fifty', 3352: 'self', 3353: 'employed', 3354: 'studious', 3355: 'hedgehogs', 3356: 'print', 3357: 'torn', 3358: 'grammar', 3359: 'smooth', 3360: 'closet', 3361: 'folks', 3362: 'hail', 3363: 'boiling', 3364: 'bathe', 3365: 'confirm', 3366: 'exclude', 3367: 'collect', 3368: 'dropped', 3369: 'tests', 3370: 'flipped', 3371: 'fractured', 3372: 'roommate', 3373: 'nosebleed', 3374: 'toothache', 3375: 'blonde', 3376: 'cabin', 3377: 'passport', 3378: 'appetite', 3379: 'cameras', 3380: 'nephews', 3381: 'jog', 3382: 'unlocked', 3383: 'l', 3384: 'races', 3385: 'kakogawa', 3386: 'yokohama', 3387: 'identity', 3388: 'butterflies', 3389: 'lentil', 3390: 'pot', 3391: 'roast', 3392: 'other', 3393: 'envelope', 3394: 'doubted', 3395: 'plowed', 3396: 'field', 3397: 'repeated', 3398: 'cigarettes', 3399: 'spilled', 3400: 'psychology', 3401: 'suggest', 3402: 'boyfriend', 3403: 'challenge', 3404: 'tiger', 3405: 'fond', 3406: 'coerced', 3407: 'farm', 3408: 'photographer', 3409: 'july', 3410: 'extremely', 3411: 'singapore', 3412: 'gaining', 3413: 'beginning', 3414: 'clients', 3415: 'sock', 3416: 'photogenic', 3417: 'older', 3418: 'rut', 3419: 'such', 3420: 'crybaby', 3421: 'terribly', 3422: 'summoned', 3423: 'gained', 3424: 'woken', 3425: 'ignorance', 3426: 'bliss', 3427: 'snake', 3428: 'belonged', 3429: 'sense', 3430: 'worthless', 3431: 'handcrafted', 3432: 'catchy', 3433: 'common', 3434: 'distraction', 3435: 'proven', 3436: 'herring', 3437: 'typo', 3438: 'corner', 3439: 'booby', 3440: 'crowded', 3441: 'merely', 3442: 'midnight', 3443: 'stinky', 3444: 'jam', 3445: 'jar', 3446: 'profile', 3447: 'promises', 3448: 'laws', 3449: 'falling', 3450: 'lesson', 3451: 'rephrase', 3452: 'delusion', 3453: 'lilies', 3454: 'luckily', 3455: 'mars', 3456: 'moons', 3457: 'excluded', 3458: 'aunt', 3459: 'bookworm', 3460: 'final', 3461: 'sealed', 3462: 'east', 3463: 'orange', 3464: 'forever', 3465: 'perhaps', 3466: 'tax', 3467: 'guns', 3468: 'bandage', 3469: 'postcard', 3470: 'kissing', 3471: 'twenty', 3472: 'leaped', 3473: 'note', 3474: 'lonesome', 3475: 'frankly', 3476: 'alcoholic', 3477: 'meeting', 3478: 'somalia', 3479: 'godmother', 3480: 'slander', 3481: 'somehow', 3482: 'knocking', 3483: 'harassing', 3484: 'fuss', 3485: 'strive', 3486: 'dear', 3487: 'sharing', 3488: 'intriguing', 3489: 'total', 3490: 'unnecessary', 3491: 'attempt', 3492: 'axle', 3493: 'case', 3494: 'purring', 3495: 'crow', 3496: 'damage', 3497: 'disco', 3498: 'doors', 3499: 'flame', 3500: 'fog', 3501: 'lifted', 3502: 'rigged', 3503: 'caved', 3504: 'rallied', 3505: 'frozen', 3506: 'ocean', 3507: 'rocket', 3508: 'spider', 3509: 'stars', 3510: 'setting', 3511: 'tires', 3512: 'squealed', 3513: 'waves', 3514: 'smashing', 3515: 'peered', 3516: 'foreigners', 3517: 'handcuffed', 3518: 'shaggy', 3519: 'triangle', 3520: 'bedroom', 3521: 'mission', 3522: 'plagiarism', 3523: 'remarkable', 3524: 'typical', 3525: 'surprising', 3526: 'bigger', 3527: 'tighten', 3528: 'march', 3529: 'th', 3530: 'appears', 3531: 'unhurt', 3532: 'pork', 3533: 'chewed', 3534: 'comforted', 3535: 'contacted', 3536: 'counts', 3537: 'described', 3538: 'combat', 3539: 'typhus', 3540: 'lemonade', 3541: 'dried', 3542: 'fled', 3543: 'freed', 3544: 'glared', 3545: 'curly', 3546: 'enemies', 3547: 'regrets', 3548: 'honked', 3549: 'cameraman', 3550: 'celebrity', 3551: 'chauffeur', 3552: 'communist', 3553: 'nerd', 3554: 'socialist', 3555: 'terrorist', 3556: 'employee', 3557: 'dating', 3558: 'independent', 3559: 'insensitive', 3560: 'intoxicated', 3561: 'mischievous', 3562: 'pessimistic', 3563: 'obedient', 3564: 'lacrosse', 3565: 'redheads', 3566: 'appalled', 3567: 'dreadful', 3568: 'youthful', 3569: 'parked', 3570: 'proposed', 3571: 'reassured', 3572: 'relies', 3573: 'returns', 3574: 'ripped', 3575: 'sliced', 3576: 'stranger', 3577: 'disfigured', 3578: 'humiliated', 3579: 'hypnotized', 3580: 'ostracized', 3581: 'ballistic', 3582: 'nasa', 3583: 'volume', 3584: 'australians', 3585: 'frustrated', 3586: 'wheat', 3587: 'strategy', 3588: 'watchdog', 3589: 'permission', 3590: 'tools', 3591: 'sacrifice', 3592: 'tent', 3593: 'mud', 3594: 'shelter', 3595: 'bodybuilders', 3596: 'fixing', 3597: 'honeymooning', 3598: 'couple', 3599: 'spotted', 3600: 'tricked', 3601: 'usa', 3602: 'amused', 3603: 'pumpkin', 3604: 'coincidence', 3605: 'spill', 3606: 'whales', 3607: 'narrow', 3608: 'hidden', 3609: 'vase', 3610: 'prevent', 3611: 'visiting', 3612: 'float', 3613: 'scar', 3614: 'evicted', 3615: 'elephants', 3616: 'contented', 3617: 'haircut', 3618: 'gal', 3619: 'racists', 3620: 'charismatic', 3621: 'cooperating', 3622: 'egotistical', 3623: 'favorite', 3624: 'princess', 3625: 'purse', 3626: 'zipper', 3627: 'gym', 3628: 'enjoying', 3629: 'prophet', 3630: 'least', 3631: 'athens', 3632: 'firm', 3633: 'depressing', 3634: 'overrated', 3635: 'chew', 3636: 'everywhere', 3637: 'virus', 3638: 'elaborate', 3639: 'increasing', 3640: 'disconnect', 3641: 'b', 3642: 'motto', 3643: 'oysters', 3644: 'surfing', 3645: 'salad', 3646: 'questions', 3647: 'slam', 3648: 'bait', 3649: 'embrace', 3650: 'faults', 3651: 'sometimes', 3652: 'sorrows', 3653: 'property', 3654: 'sight', 3655: 'toothpick', 3656: 'breaks', 3657: 'hatred', 3658: 'thimble', 3659: 'mt', 3660: 'fuji', 3661: 'genoa', 3662: 'wales', 3663: 'commited', 3664: 'opposite', 3665: 'rage', 3666: 'sigh', 3667: 'grabbed', 3668: 'lady', 3669: 'wisely', 3670: 'picassos', 3671: 'tightly', 3672: 'dealer', 3673: 'indeed', 3674: 'penance', 3675: 'kilos', 3676: 'washing', 3677: 'led', 3678: 'sober', 3679: 'while', 3680: 'adventures', 3681: 'cantaloupe', 3682: 'cardiff', 3683: 'morocco', 3684: 'bug', 3685: 'suspicious', 3686: 'response', 3687: 'muttered', 3688: 'stays', 3689: 'cages', 3690: 'popped', 3691: 'reached', 3692: 'shoe', 3693: 'shined', 3694: 'ohio', 3695: 'buried', 3696: 'queue', 3697: 'captain', 3698: 'defeat', 3699: 'lab', 3700: 'hunter', 3701: 'kisser', 3702: 'tipsy', 3703: 'scriptwriter', 3704: 'fifties', 3705: 'known', 3706: 'sentry', 3707: 'cheeks', 3708: 'whole', 3709: 'escapes', 3710: 'nhk', 3711: 'magnets', 3712: 'concert', 3713: 'pluck', 3714: 'india', 3715: 'peeling', 3716: 'kanji', 3717: 'sms', 3718: 'hasty', 3719: 'pencils', 3720: 'permit', 3721: 'exactly', 3722: 'cleaned', 3723: 'cleared', 3724: 'fence', 3725: 'england', 3726: 'daydream', 3727: 'decorated'}\n",
            "3727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSTMdEJF0pKI",
        "colab_type": "code",
        "outputId": "3a4a74ea-54db-47a8-83a7-1a717662261b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(max_length_inp,max_length_targ)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05i8WUQT0pKN",
        "colab_type": "code",
        "outputId": "3b215758-e2f7-40b3-be9f-382b4f90e5d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"input tensor shape : \",input_tensor.shape)\n",
        "print(\"target tensor shape : \",target_tensor.shape)\n",
        "sent = en[1011]\n",
        "print(sent,input_tensor[1011])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input tensor shape :  (20000, 16)\n",
            "target tensor shape :  (20000, 10)\n",
            "<start> he s broke . <end> [   1   13  229 1276    3    2    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ucRpKUo0pKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrMEbrgJ0pKZ",
        "colab_type": "code",
        "outputId": "4ee405d1-0ce2-4d0e-9658-517205868f08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16000 16000 4000 4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayaYd_fn0pKf",
        "colab_type": "code",
        "outputId": "16b645af-c389-42a9-e26e-1de09d3b5483",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#This is what our model is going to learn\n",
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "1133 ----> muestrame\n",
            "93 ----> esa\n",
            "291 ----> lista\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "269 ----> show\n",
            "17 ----> me\n",
            "19 ----> that\n",
            "1070 ----> list\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRmaikr_0pKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2dB-PgW0pKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train) #training set size\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "#Creates a Dataset whose elements are slices of the given tensors.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "#Combines consecutive elements of this dataset into batches.\n",
        "#eg dataset = tf.data.Dataset.range(8) \n",
        "# dataset = dataset.batch(3) \n",
        "# list(dataset.as_numpy_iterator()) \n",
        "# [ array([0,1,2]), array([3,4,5]) , array([5,6,7])]\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2ZwrCC0pKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i7_WbqN0pK0",
        "colab_type": "code",
        "outputId": "eacb0671-f779-4109-924d-35d87964f7cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# so dataset loads 32 examples at a time for both input and output\n",
        "# its a pointer to point to next batch when needed.\n",
        "print(dataset)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((32, 16), (32, 10)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UaxPkM50pK6",
        "colab_type": "code",
        "outputId": "b2aa6547-d5f7-4414-e565-b30e5a7b130a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(32), Dimension(16)]),\n",
              " TensorShape([Dimension(32), Dimension(10)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmPlb5vH0pK_",
        "colab_type": "code",
        "outputId": "cee2ae11-0548-4c23-e8c8-12624eb10725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(example_input_batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   1  688   10  142    3    2    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   40 5195    3    2    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   13    7 1342    3    2    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1 4254    9 1167    3    2    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    5   47   75   12   89    4    2    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1 2457   49 2458    3    2    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1  297 1611    3    2    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   15 2811    3    2    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    5   14  286    4    2    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   15 1836    7  519    3    2    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1  244  322   10 2158    3    2    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   57   17  114 6070    3    2    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    6   13 1298    3    2    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   17 2368   12  496    3    2    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   11   70 1627    9  239    3    2    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    5   46   16  431  940    4    2    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   76   13    3    2    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    8 2612    3    2    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   56    8   23   97   10 1406    3    2    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    6 1103    3    2    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1  225  229   20    3    2    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   93    7   15   53   16    6    3    2    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    8   11   36 7167    3    2    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1  248  106   10  106    3    2    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    6 6821   25 2230 3293    3    2    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1  285  338    3    2    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1 4569   10   14  341    3    2    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    5  634    6   10   48    4    2    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   66  570   10    6    3    2    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1   11 1363    3    2    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    5   62   13   20  355    4    2    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1 4649    3    2    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]], shape=(32, 16), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frqAhmQV0pLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuLqs00r0pLK",
        "colab_type": "text"
      },
      "source": [
        "## Making the model\n",
        "\n",
        "![first_image](images/1.jpg)\n",
        "![second_image](images/2.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHQaXF9i0pLM",
        "colab_type": "raw"
      },
      "source": [
        "FC = Fully connected (dense) layer\n",
        "EO = Encoder output\n",
        "H = hidden state\n",
        "X = input to the decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY0ME3ta0pLO",
        "colab_type": "text"
      },
      "source": [
        "## pseudo code\n",
        "> score = FC(tanh(FC(EO) + FC(H)))  \n",
        "> attention weights = softmax(score, axis = 1)  \n",
        "> context vector = sum(attention weights * EO, axis = 1)  \n",
        "> embedding output = The input to the decoder X is passed through an embedding layer.  \n",
        "> merged vector = concat(embedding output, context vector)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmRIFAol0pLR",
        "colab_type": "text"
      },
      "source": [
        "**return_sequences** -> give hidden state for each time step  \n",
        "**return_state** -> gives (in case of lstm) [hidden state, hidden state, cell state for last time step].  \n",
        "**both of them** -> (in case of lstm)[hidden state for all time step, hidden state for last time step, cell state for last time step]  \n",
        "(in case of gru)[hidden state for all time steps, hidden state for final timestep]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Ygwhtw0pLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        #takes input and returns h and y\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJzLv2P00pLZ",
        "colab_type": "code",
        "outputId": "ee3b2638-2b15-4071-d529-27ba8b7fb3b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(vocab_inp_size)\n",
        "print(embedding_dim,units,BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7177\n",
            "256 1024 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhYgC9v10pLd",
        "colab_type": "text"
      },
      "source": [
        "### sample output is hidden state of encoder for all timesteps that is {h1,h2,h3...h16}\n",
        "### sample_hidden is output or hidden state for last time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddGof_wu0pLf",
        "colab_type": "code",
        "outputId": "337af535-383c-40d2-f23e-c3dd999581d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# encoder is a class instance having \n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# not sure how this line is working..\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (32, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (32, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxbfkV_l0pLl",
        "colab_type": "code",
        "outputId": "184d15a9-94e5-4b8e-9eb0-3c51fa229077",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"shape of sample_hidden\",sample_hidden.get_shape())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of sample_hidden (32, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF56_kQJ0pLp",
        "colab_type": "text"
      },
      "source": [
        "## ATTENTION\n",
        "### the attention weights for each example is different and to calculate that we use a feed forward neural network to get attention weights for each word in the sentence.\n",
        "### here attention weights will be of the shape  (n,1,10)  \n",
        "n is number of training examples  \n",
        "10 is length of each sentence after being encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AafCX4sS0pLq",
        "colab_type": "text"
      },
      "source": [
        "### hidden size = units = 1024\n",
        "### value is actually {h1,h2,h3...h16}  : shape == (batch_size, max_len, hidden size)\n",
        "\n",
        "### query is last hidden state of decoder that is h(t-1) \n",
        "### hi is hidden state , a vector of 1024 elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe_AOor30pLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        \n",
        "\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size) \n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVjh8iKZ0pL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu8Fx0Iw0pL9",
        "colab_type": "code",
        "outputId": "15b7b5f7-aa19-4632-ec2b-579e30517373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# this is just testing if attention is working or not\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (32, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (32, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq0lelzP0pMB",
        "colab_type": "code",
        "outputId": "c51b39c1-83c0-4571-f701-6e4495079191",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(sample_hidden)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.014413   -0.00868872  0.00984907 ...  0.01606963  0.00927834\n",
            "   0.02894218]\n",
            " [-0.01446477 -0.00865362  0.00996099 ...  0.01611441  0.00924119\n",
            "   0.02900679]\n",
            " [-0.01442111 -0.00872763  0.00987569 ...  0.01607806  0.0092692\n",
            "   0.02892711]\n",
            " ...\n",
            " [-0.01448955 -0.00864538  0.00989982 ...  0.016149    0.00918997\n",
            "   0.02899837]\n",
            " [-0.01425114 -0.00896944  0.00969348 ...  0.01577897  0.00918904\n",
            "   0.02860881]\n",
            " [-0.01451165 -0.00861996  0.01003067 ...  0.01614274  0.0092021\n",
            "   0.02900198]], shape=(32, 1024), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Rh02IR0pMG",
        "colab_type": "text"
      },
      "source": [
        "## decoder class\n",
        "all the class variables are as defined above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7HYsCtT0pMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        # enc_output is the list of all the hidden states\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngN8uPp_0pML",
        "colab_type": "text"
      },
      "source": [
        "### state is hidden state of gru for that time step\n",
        "### output and state are the same thing here , as this is only for one time step\n",
        "### output is :  Tensor(\"decoder_2/gru_5/transpose_1:0\", shape=(32, 1, 1024), dtype=float32)\n",
        "### state is :  Tensor(\"decoder_2/gru_5/while/Exit_3:0\", shape=(32, 1024), dtype=float32)\n",
        "notice the difference in shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE_RH93Q0pMN",
        "colab_type": "text"
      },
      "source": [
        "### testing if decoder class is working\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfVtqV_o0pMP",
        "colab_type": "code",
        "outputId": "883bfe17-13aa-4dd3-acf0-2722d60fe4c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (32, 3728)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYK3knvn0pMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkOkPxK60pMa",
        "colab_type": "text"
      },
      "source": [
        "### as we cannot use the model funtion from keras here as mentioned above we'll have to write and combine loss function , optimizer and model.fit manually.\n",
        "\n",
        "![third_image](images/3.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfa3quU-0pMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "'''\n",
        "In the snippet below, there is a single floating point value per example for\n",
        "`y_true` and `# classes` floating pointing values per example for `y_pred`.\n",
        "The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n",
        "`[batch_size, num_classes]`.\n",
        "'''\n",
        "#Computes the crossentropy loss between the labels and predictions.\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "# only considering cross entropy for incorrectly classified samples.\n",
        "def loss_function(real, pred):\n",
        "    # tf.math.logical - Returns the truth value of NOT x element-wise.\n",
        "    # tf.math.equal - Returns the truth value of (x == y) element-wise.\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    # loss as defined in the image above\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zpVkGjN0pMf",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6P1MX520pMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B241DBs60pMl",
        "colab_type": "code",
        "outputId": "8f6083aa-f830-47b0-da9d-7e1b99a0c229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for (batch, (inp, targ)) in enumerate( dataset.take(steps_per_epoch) ):\n",
        "    print(targ)\n",
        "    print(targ.shape)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   1  160 1459    3    2    0    0    0    0    0]\n",
            " [   1    5   18  801    3    2    0    0    0    0]\n",
            " [   1   20    8   69    3    2    0    0    0    0]\n",
            " [   1  954   14  964    3    2    0    0    0    0]\n",
            " [   1   22    5   31   11  111    6    2    0    0]\n",
            " [   1  779    3    2    0    0    0    0    0    0]\n",
            " [   1   26   91  206    3    2    0    0    0    0]\n",
            " [   1   29   71  754    3    2    0    0    0    0]\n",
            " [   1   34   10  120   40    6    2    0    0    0]\n",
            " [   1 1165    8  503    3    2    0    0    0    0]\n",
            " [   1    4  168   33   21  226    3    2    0    0]\n",
            " [   1   26   74  742    3    2    0    0    0    0]\n",
            " [   1    7   10 2095    3    2    0    0    0    0]\n",
            " [   1   85 2217   27  113    3    2    0    0    0]\n",
            " [   1    4   71   25  268  468    3    2    0    0]\n",
            " [   1   50  490   44    5   33    6    2    0    0]\n",
            " [   1   19    8   82    3    2    0    0    0    0]\n",
            " [   1   30   13 1212    3    2    0    0    0    0]\n",
            " [   1    7  101   13  217    5    3    2    0    0]\n",
            " [   1    7 1533    3    2    0    0    0    0    0]\n",
            " [   1   33   40 1672   17    3    2    0    0    0]\n",
            " [   1   19   10    7   10  164    3    2    0    0]\n",
            " [   1    4   24   13  273    9    3    2    0    0]\n",
            " [   1   67   93    9  780    3    2    0    0    0]\n",
            " [   1    7 2149   11 2150    3    2    0    0    0]\n",
            " [   1    4   24  166  410    3    2    0    0    0]\n",
            " [   1    4 1241    9  365    3    2    0    0    0]\n",
            " [   1   44    7  295   21   52    6    2    0    0]\n",
            " [   1   67  134    7    3    2    0    0    0    0]\n",
            " [   1    4 1469    3    2    0    0    0    0    0]\n",
            " [   1   73    8   25  639    6    2    0    0    0]\n",
            " [   1   42   10 2652    3    2    0    0    0    0]], shape=(32, 10), dtype=int32)\n",
            "(32, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hezjeujC0pMs",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9fzkNfC0pMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flag = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daxa8Bz60pM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    '''\n",
        "    inp is encoded input sentence \n",
        "    targ is encoded output sentence(batchsize,len_of_target_word)\n",
        "    enc_hidden is zero vector of shape (batchsize,units)\n",
        "    '''\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        #recoder or tracks variable values\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # targ_lang.word_index['<start>'] - returns dictionary value of start token\n",
        "        # dec_input is batch_size\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            #enc_output is list of all hidden states of encoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            \n",
        "            # this is simply the loss between t th predicted word and its prediction\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            # expand_dims is to change dimention such that it can act as timesteps\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    \n",
        "    # explanationn of this part https://www.tensorflow.org/guide/effective_tf2\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    \n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NROWVZyG0pM6",
        "colab_type": "text"
      },
      "source": [
        "## operation code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMWEDCKS0pM7",
        "colab_type": "code",
        "outputId": "db8abaf3-5463-4d67-8c77-c77c67670584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # returns a zero vector of shape(batch_size,units)\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    # dataset is tf.data.Dataset() object.\n",
        "    for (batch, (inp, targ)) in enumerate( dataset.take(steps_per_epoch) ):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1 Loss 1.9837\n",
            "Time taken for 1 epoch 64.3696551322937 sec\n",
            "\n",
            "Epoch 2 Loss 1.3402\n",
            "Time taken for 1 epoch 43.472676515579224 sec\n",
            "\n",
            "Epoch 3 Loss 0.9677\n",
            "Time taken for 1 epoch 43.29481768608093 sec\n",
            "\n",
            "Epoch 4 Loss 0.6615\n",
            "Time taken for 1 epoch 43.56736493110657 sec\n",
            "\n",
            "Epoch 5 Loss 0.4284\n",
            "Time taken for 1 epoch 43.27933359146118 sec\n",
            "\n",
            "Epoch 6 Loss 0.2765\n",
            "Time taken for 1 epoch 43.56578516960144 sec\n",
            "\n",
            "Epoch 7 Loss 0.1818\n",
            "Time taken for 1 epoch 43.716572761535645 sec\n",
            "\n",
            "Epoch 8 Loss 0.1280\n",
            "Time taken for 1 epoch 43.63137888908386 sec\n",
            "\n",
            "Epoch 9 Loss 0.1044\n",
            "Time taken for 1 epoch 43.20781922340393 sec\n",
            "\n",
            "Epoch 10 Loss 0.0861\n",
            "Time taken for 1 epoch 43.60098576545715 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MvHbnXxOzUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucOohbQEO1jd",
        "colab_type": "text"
      },
      "source": [
        "## Evaluator\n",
        "The evaluate function is similar to the training loop, except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZfulsHr0pNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  # this is for the heat map\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  #convert input to its embeddings\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_inp,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # we'll keep appending the predicted word in this\n",
        "  result = ''\n",
        "\n",
        "  # got all the hidden states and last encoder hidden state\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # for first timestep decoder hidden state = encoder hidden state\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  # keep feeding words in decoder for no of words in target sentence\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                          dec_hidden,\n",
        "                                                          enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    # max value from probability of all words in target sentence\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    print(predictions.shape,predictions)\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jauhunjBZ_V_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stinglXdax8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSjdil8ta9JD",
        "colab_type": "text"
      },
      "source": [
        "# Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAV5x7eAa3Uq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a46ede37-542a-4385-fcc1-cc30410ba628"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcfc02b7c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf3Fur9GbCp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrJ4hztybDqP",
        "colab_type": "text"
      },
      "source": [
        "## checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8vEmQFa49V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "outputId": "c9c17dff-a510-4b8a-944f-570059df45e5"
      },
      "source": [
        "translate(u'hace mucho frio aqui.')\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3728) tf.Tensor([[-5.240873  -4.607679   0.5433209 ... -3.7884514 -4.172498  -5.356888 ]], shape=(1, 3728), dtype=float32)\n",
            "(1, 3728) tf.Tensor(\n",
            "[[-2.972187   -2.4347107  -0.71215403 ... -2.887904   -0.64589614\n",
            "  -3.2920468 ]], shape=(1, 3728), dtype=float32)\n",
            "(1, 3728) tf.Tensor([[-9.27494   -8.359457  -5.9413548 ... -8.206115  -7.933142  -9.498923 ]], shape=(1, 3728), dtype=float32)\n",
            "(1, 3728) tf.Tensor([[-5.6119537 -4.6771717 -5.499144  ... -3.3513527 -5.252321  -4.874863 ]], shape=(1, 3728), dtype=float32)\n",
            "(1, 3728) tf.Tensor(\n",
            "[[-10.703559   -9.318482    1.0674783 ...  -9.366171   -9.715652\n",
            "   -9.991129 ]], shape=(1, 3728), dtype=float32)\n",
            "(1, 3728) tf.Tensor([[ 1.5591072  1.3505776 23.610983  ... -0.1415377 -2.9373422  1.4197962]], shape=(1, 3728), dtype=float32)\n",
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: he s very hot . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAJwCAYAAAAjo60MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de7ztdV3n8fcHDpeA0BAv6ERS3vCK\ncNIUSxw1Kh0rp9EUDXVGynTM0cZqnNKcwfJWY1mTWIMpYF7SUbOxvIZ5GUJtjLwgIeIlBBJT5A6f\n+eO3sM1mHzj7cNi/79rn+Xw8zsO1f2udtT/n5+Gs1/5dq7sDAMD8dpt7AAAAJsIMAGAQwgwAYBDC\nDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMJsQFV156p6X1Xda+5ZAICNI8zGdGyS\no5I8ZeY5AIANVG5iPpaqqiTnJHl3kn+T5PbdffWsQwEAG8IWs/EcleQ7kzwzyVVJfmzWaQCADSPM\nxnNskjd39yVJ/mTxNQCwC7ArcyBVtW+Sf0zyiO7+YFUdluQjSQ7q7q/POx0AcHOzxWws/zbJhd39\nwSTp7r9N8rkkPz3rVACwRKpq36r6maq6xdyzrJcwG8sTk5y0atlJSZ608aMAwNJ6TJITM32uLhW7\nMgdRVd+d5PNJDu3uz61Y/q8ynaV59+4+c6bx2ISq6t5JfjHJ3ZN0kk8leWl3nzHrYAA3UVW9P8lt\nk1zS3Vvnnmc9hBnsgqrqUUnekuSDSf56sfhBi1+P7u53zDUbwE1RVXdMcmaS+yX5aJLDu/tTc860\nHsJsIFV1cJIv9hr/p1TVwd197gxjsQlV1SeTvLW7n79q+QuT/Hh332eeyQBumqr61SRHdfdDq+ot\nST7X3b8091zbyzFmY/l8kluvXlhVt1o8BzvLXZK8bo3lr0ty1w2eBWBn+pn8y79vJyc5ZnHx9qUg\nzMZSmY71WW2/JJdt8CxsbucnOWKN5Uck+eoGzwKwU1TVA5MclOTNi0XvSLJPkofNNtQ6bZl7AJKq\n+p3Fw07yG1V1yYqnd8+0n/xvN3wwNrNXJ3lVVd0pyYcXy47MdDLAS2ebCuCmOTbJ27r74iTp7iuq\n6o2Zrm7w7jkH216OMRvA4uyRJHlwpgvKXrHi6SsynZX5spVna8JNsdis/6wkz0ly+8Xir2SKst9Z\n6zhHgJFV1V5JzkvyuO5+14rlD0ryF0lue22wjUyYDWLxQfnGJE/p7m/OPQ+7jqr6ziTx9w5YZlV1\nYKb7S5/U3deseu4JSd7T3efNMtw6CLNBVNXumY4ju88yndYLAOw8jjEbRHdfXVVfSLLn3LOw+VXV\nAUmOT/LQJLfJqhOBunv/OeYC2NUJs7H8tyS/WVVP6O4L5x6GTe2Pktw3yQmZji2z6RxYSlX1+Wzn\nv2Hd/b038zg3mV2ZA6mqv0tySJI9knwpybdWPt/d955jLjafqvpGkod39/+dexaAm6KqnrPiy/2S\nPDvJaZlOpkuSB2S6usHLu/uFGzzeutliNpY33/hLYKc4P8nwZycB3Jjufvm1j6vqNUle3N0vWvma\nqvqVJPfY4NF2iC1msAuqqscmeUySY5fh9HGA7bHYG3B4d5+1avmdknx8GY6ftcWMTaGqfj7J0zPt\nCr5nd59dVb+c5OzufuO8041hsat85U9ihyQ5f3HSyZUrX2u3ObCkvpXkqCRnrVp+VJJLVr94RMJs\nIFW1Z5LnJXlckoMzHWv2bd29+xxzja6qnpXkuUlenOQ3Vzz15STPyHR9OOwqBza/307ye1W1NclH\nF8t+INMdAV4w11DrYVfmQKrqxUkem+Q3Mv3l+q9J7pjkp5P8ane/ar7pxlVVn0nynO5+Z1V9M9O1\n4M6uqnskObW7bzXziLBLqarDk/xtd1+zeLxN3f3xDRqLXURVPSbJLyQ5dLHo00lesSx7T4TZQBan\n/D6tu9+1CIzDuvsfquppSR7a3T8184hDqqpLk9ytu7+wKszukunDYZ+ZRxxOVT04Sbr7r9ZY3t19\n6iyDsSlU1TVJbtfd5y8ed5Ja46VtTwBcl12ZY7ltkmuv+n9xklsuHr8r02461nZ2ksOTfGHV8h/L\nv6xPruu3k6x12vj+mTb3H7Gh07DZHJLkghWPYcNV1S1z/Ytnf22mcbabMBvLuZluKH1upgMXj07y\nsUzXYLl0xrlG97Ikr6yqfTL9VP6AqnpipuPOnjLrZOO6a5L/t8byMxbPwQ7r7i+s9RhublX1PUn+\nINPB/ivvpFOZttwOv4VWmI3lrZlukfPRJK9I8vqqemqSOyR56ZyDjay7T6yqLUlelGSfJK/LdDX7\nZ3b3G2YdblyXJjkoyedXLb9Dkis2fhw2K8eYscFOzLS36d9nSe9q4hizgVXV/ZMcmeTM7v6zuedZ\nBlV1YJLduvv8uWcZWVWdnOnM30d190WLZQckeVuSL3X34+acj81jG8eYffuDxzFm7ExVdXGSH+ju\nM+aeZUcJs4FU1Q8l+XB3X7Vq+ZYkD3RA9toWZ1/u3t2fXLX83kmu6m7Hma1SVQclOTXTDcyvXW/3\nznRHgAd391fmmo3NZbFraaU9Mt2n9XlJfqW7/8/GT8Vmtbhe45O6+2Nzz7KjhNlAqurqJAet3tpT\nVbdKcr6fLNdWVR9K8nvdfcqq5T+d5Bnd/aB5Jhvb4pi8Y5Ictlj0iSSndPdSXIRxLlX1r5PcPdNW\nn0919/tnHmkpVdUPJ3l+dx859yxsHov/Pn85yc+vvvr/shBmA1ls8r9td1+wavldkpy+DLeSmMPi\nEhn3XeMWHN+X6RYct5hnMjaTqrpDpuNAj8h07EoynaxzepKftJVxfarqzpkuZ7Pv3LOweSw+D/bK\ndJD/5UmuswdqGT5HHfw/gKp6++JhJzmpqi5f8fTuSe6Z5MMbPtjyuDrJWvH1XVn72km7vKp69A09\n391v2ahZlsjvZPq7dqfu/nySVNX3Jjlp8ZzrDK5hcezidRZlOvHkBUk+u+EDsdk9Y+4BbipbzAZQ\nVScuHh6b6fZBKy+NcUWSc5K8ursv3ODRlkJVvS3TB+a/6+6rF8u2JHlTkj26+5FzzjeixdbZtXTi\ngOy1LG6OfNTqswgXt355ry2za1tx8P91Fif5YpLHdvdHr/+7YNdli9kAuvvJSVJV5yR5WXd/a96J\nls5zk/x1krOq6q8Xyx6UZL8kPzTbVAPr7utcdHERsvfNdFmW580y1HJY6ydZP93esIes+vqaTBef\nPWv1iU6wM1TVbZM8Mcn3Zbqd4YVVdWSSr1y7tXtktpgNpKp2S5Luvmbx9e2SPDLTAcZ2Zd6AxVmG\nz8h1D2T/fcf9rE9VPTDJ/+zu+8w9y2iq6q1Jbp3kcd39xcWyg5OcnOSC7r7B3cPAza+qjkjy3kzX\naLxHptv1nV1VL0hyl+5+/JzzbQ9hNpCq+j9J3tXdr6iq/ZJ8Jsm+mbb8/Pvufu2sA7LpVdXdk5zW\n3fvNPctoquq7k7w90zGfKw/+/7tM14P70lyzjWxxGaDt4pJA3FRV9f4kp3b381fdO/kBSf6ku1df\nvmU4dmWOZWum3XJJ8ugk38h0n7ljkvxiEmF2A6rq9pkumrryNhz+sV/DGldjv/aA7F/KtLWRVbr7\ni4v19rAkd1ss/nR3v2fGsZbBB/Ivu3uvPRln9dfXLnNsIzfVEZmu+r/aP2a6H/XwhNlY9kvy9cXj\nH07y1u6+sqrel+T35htrbIsgOyXT8WTXXmF85aZg/9hf3+m5/tXYk+l2YO4vug097WJ49+IX2+eR\nme5ne3ySjyyWPSDJf8n0g6iD/9mZLs10Rv5qd8t0Ae3hCbOxnJvkyKp6R6YbmP+7xfIDkrjo57b9\nj0xnZd49yd8k+ZFMPxm9MMl/mnGukR2y6utrMh0nddkcw4yqqp6d6VjFyxaPt6m7f2uDxlo2/y3J\nL3T3ypg9u6rOT/KS7r7vTHOxOb0tyfOr6trPz66qOyZ5cZI/nWuo9XCM2UCq6meTvDLJxUm+kOTw\n7r6mqp6Z5Ce6+1/POuCgquqrSR7R3acvLmmwtbvPrKpHZDoj5wdmHnFIizOXjsx0W6brnKXZ3b8/\ny1CDqarPZ/r79E+Lx9vS3f29GzXXMqmqSzP9W/bpVcvvnuRj3f0d80zGZlRV+yf580y3mNs3yXmZ\nflD/cJIfXYarHgizwSzOKDk4ybu7++LFskck+Xp3f2jW4Qa1iLF7d/c5i0uOPKG7/7qqDkny9929\nz7wTjqeqnpDkDzPtyrwo19312919+1kGY9OpqtOTnJXkyd196WLZdyQ5MdPFerfOOR+b0+LWTIdn\n+qHz48t0LKhdmYOoqltkiosPJll989WvJ3Ej7m37TKbjB85J8rdJfq6qvpjk6Um+PONcIzs+yUuS\nvNC1pG5cVe2R6Vp5P9Pdrla/Pk9L8mdJvlxVn1wsu1emww8eMdtUbDorP0e7+31J3rfiuSMzXXrq\notkG3E62mA2iqr4z01kjR6/cMlZV90lyWpI7uPL/2qrqmExX+H/N4qy5dyU5MNN90o7t7jfOOuCA\nquqiJEd099lzz7IsFsdEPai7z5x7lmVTVfsmeXySQxeLPp3klGXYrcTy2Cyfo8JsIFV1cpKLu/tn\nVyx7WaaL4j1qvsmWS1Xtk2kL2rnL8B/hHKrqlUk+292/O/csy6KqXpok3f2f555l2SzuLHG/rH05\nG5cBYqfZDJ+jwmwgVXV0ktcnuV13X7G4E8CXkjzDTaVvWFU9NslDs/aB7EvxH+NGqqo9k/zvTPdi\n/bskV658vrtfOMdcI6uq3890TcHPZzrc4Dpbe7r7mXPMNbqquluSd2Q6E7gy7cLckunv3OXdvf+M\n47HJbIbPUceYjeXdma7B8sgkb8kUGntm+keNbVhsyXhWkvdnuiK7nzZu3M9muqzIhUnulFUH/2e6\n1Mgub3HV+g8vjsM7NMm1NzBffQamv3Pb9j8yhexhmc6QOyzJLZL8zyT/dca52JyW/nPUFrPBVNWL\nk9y1u3+iql6b5Jvd/fS55xrZ4nIZT+/uN889y7JYHC/1G93923PPMrKqujrJQd19flWdneT7u/uf\n5p5rmVTVPyV5cHefUVX/nOR+3f3Zqnpwkt/t7nvPPCKbzLJ/jtpiNp7XJvnY4ubIP5mp9rlhu2U6\nG5Ptt3um+z5ywy7KtAvu/CR3zKrd5GyXyr9cIPuCJHdI8tlMu5fuNNdQbGpL/Tlqi9mAFtf9uTTJ\ngd196I29fldXVccnubK7XzD3LMticTDsNxxLdsOq6lVJjs10ptfBmWLi6rVe6wKza6uqU5P8dne/\ntapOSXKrJC9K8tRMlzawxYydbpk/R20xG9NrMx2X8by5BxlVVf3Oii93S3JMVT08ySdz/QPZHZR9\nffsk+Q+LA2Wts237uUxbFu+c5LcyXRT1m7NOtHyOz3QF9mQ6puydmY4HvTDJY+YaaplV1aeT3Lm7\nfYZv29J+jvo/dUwnZboJ64lzDzKwe636+tpdmXdbtdwm4bUdmuQTi8fW2TYsblr+zuTb10J6eXcL\ns3Xo7r9Y8fjsJIdW1QFJLmq7bHbU72Xa8si2Le3nqF2ZAACDcCArAMAghBkAwCCE2cCq6ri5Z1hG\n1tv6WWc7xnrbMdbb+llnO2YZ15swG9vS/YUahPW2ftbZjrHedoz1tn7W2Y5ZuvUmzAAABrHLn5W5\nZ+3Ve3/7EjtjuTKXZ4/sNfcYa6rdxm36K/qy7Fl7zz3G9ex/6JU3/qKZXPy1K7LfAXvOPcaaLvrK\nuPe4vvLyi7PHXvvNPcaadr9k3L9vV1x9SfbcfZ+5x7ievvyKuUfYppE/D0Y28nr7Zi66sLtvvXr5\nLn8ds72zb+6/28PmHmPp7LbfmB9GI3v4G7889whL6U9//eFzj7CUbvmxr849wtK56pwvzj3Ccupr\n5p5gKb3nmjd9Ya3l4272AADYxQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMA\ngEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBB\nCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgz\nAIBBLEWYVdUHquqVc88BAHBzWoowAwDYFQgzAIBBLFOY7VZVL6qqC6vq/Kp6WVXtliRVtWdVvbiq\nvlRVl1TV31TV0XMPDACwHssUZsckuSrJA5M8I8mzkjx28dyJSR6c5PFJ7pnkj5O8o6ruM8OcAAA7\nZMvcA6zDp7r71xaPz6yqpyZ5aFWdluRxSe7Y3ecunn9lVT0syc8m+fnVb1RVxyU5Lkn2zj43/+QA\nANthmcLsk6u+/kqS2yQ5PEkl+VRVrXx+ryTvW+uNuvuEJCckyf51QO/0SQEAdsAyhdmVq77uTLti\nd1s8/v41XnPpBswFALBTLFOYbcsnMm0xu113v3/uYQAAdtTSh1l3n1lVJyd5TVU9J8nHkxyQ5Kgk\nZ3f3W+acDwBgey19mC08Ocnzkrwkyb9K8rUkpyWxBQ0AWBpLEWbdfdQay5604vGVSV6w+AUAsJSW\n6TpmAACbmjADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiE\nMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDAD\nABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYxJa5BxhC99wTLJ1rLr54\n7hGWzutffvTcIyylez7njLlHWEr/+B/vOPcIS6fOrblHWEp9jW08O5O1CQAwCGEGADAIYQYAMAhh\nBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYA\nMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAI\nYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMIhNF2ZV9UNV\n9dGquriq/rmqTquqe849FwDAjdky9wA7U1VtSfK2JH+U5JgkeyQ5PMnVc84FALA9NlWYJdk/yS2T\nvKO7/2Gx7DOrX1RVxyU5Lkn2zj4bNx0AwA3YVLsyu/trSV6T5C+q6p1V9eyqOniN153Q3Vu7e+se\n2WvD5wQAWMumCrMk6e4nJ7l/klOTPCrJZ6vq6HmnAgC4cZsuzJKku/9fd7+4u49K8oEkx847EQDA\njdtUYVZVh1TVb1bVA6vqe6rqIUnuneRTc88GAHBjNtvB/5ckuUuSNyU5MMlXk5yc5MVzDgUAsD02\nVZh191eTPHruOQAAdsSm2pUJALDMhBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCE\nGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkA\nwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAg\ntsw9AEuqe+4Jls4BJ35k7hGW0nmv33vuEZbSu85+3dwjLJ0f/eGfnnuEpbTbBV+be4TldN7ai20x\nAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMA\nGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiE\nMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDAD\nABiEMAMAGMTSh1lV7Tn3DAAAO8OGhllVHVdVX62q3VctP6Wq3r54/G+q6mNVdVlVfb6qjl8ZX1V1\nTlW9oKr+V1V9PcnJVfW+qnrlqvfcv6ouqapHb8gfDgDgJtroLWZvSnKLJA+/dkFV7Zfkx5OcVFVH\nJzk5ySuT3CPJU5L8VJIXrXqfZyf5TJKtSf5LklcneXxV7bXiNY9LcnGSd9wsfxIAgJ1sQ8Osuy9K\n8udJjlmx+CeSXJXk7Umel+Sl3X1id/9Dd78/yS8l+bmqqhW/56+6+yXdfVZ3fy7JW5Jck+QnV7zm\nKUle291Xrp5jseXu9Ko6/cpcvlP/jAAAO2qOY8xOSvITVbXP4utjkvxpd1+W5Igkz6uqi6/9leSU\nJPsmud2K9zh95Rt29+VJXpcpxlJV90hyvyR/tNYA3X1Cd2/t7q17ZK+1XgIAsOG2zPA935lpC9mP\nV9V7kzwsydGL53ZL8uuZdnmudsGKx99a4/k/TPLJqjo4U6B9pLs/vdOmBgC4mW14mHX35VX1pkxb\nyg5Mcl6SDyye/niSu3X3WTvwvn9fVf83yVOTPCHTblEAgKUxxxazZNqd+d4khyR5fXdfs1j+wiR/\nVlVfSPLGTFvW7pnkft393O1431cn+YMkVyZ5w06fGgDgZjTXdcw+mOTLSe6eKdKSJN39F0kekeQh\nSU5b/PrlJOdu5/u+IckVSd7Y3d/cmQMDANzcZtli1t2d5I7beO4vk/zlDfzeNX/fwi2TfEe2cdA/\nAMDI5tqVuVNV1R5JbpXpemef6O4PzTwSAMC6Lf0tmRaOTPKPSR6Y6eB/AIClsym2mHX3B5LUjb0O\nAGBkm2WLGQDA0hNmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBm\nAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAA\ngxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAg9gy9wAAN+Sayy6be4SldPTtD5t7hKWz2z3n\nnmA5XXrYwXOPsJzetfZiW8wAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHM\nAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAA\nBiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYh\nzAAABiHMAAAGIcwAAAYxVJhV1Qeq6pVzzwEAMIehwuymqqonVdXFc88BALAjNlWYAQAssxHDbLeq\nelFVXVhV51fVy6pqtySpqu+qqj+uqouq6tKqek9V3WPx3FFJTkyyb1X14tcL5vtjAACsz4hhdkyS\nq5I8MMkzkjwryWMXz70myf2T/HiS+yW5JMm7quo7knx48dpLkhy0+PWyjRwcAOCm2DL3AGv4VHf/\n2uLxmVX11CQPrarTkzwqyYO7+9QkqaonJjk3yTHd/YdV9c9JurvPu6FvUFXHJTkuSfbOPjfXnwMA\nYF1G3GL2yVVffyXJbZIcmuSaJB+59onu/uckf5fk7uv5Bt19Qndv7e6te2SvmzguAMDOMWKYXbnq\n686Nz9k30ywAABtmxDDblk9nmvcB1y6oqv2T3CvJpxaLrkiy+8aPBgBw0y1NmHX355K8LcmrquoH\nq+peSU5K8o0kpyxedk6Svavq4VV1YFU5gAwAWBpLE2YLT05yWpK3L/53nyQ/0t2XJkl3fzjJHyR5\nfZILkjx3pjkBANZtqLMyu/uoNZY9acXji5IceyPv8bQkT9vZswEA3NyWbYsZAMCmJcwAAAYhzAAA\nBiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYh\nzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwA\nAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGsWXuAQBgBNec8Zm5R1hKe54x9wSbiy1mAACDEGYA\nAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACD\nEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBm\nAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAA\ngxBmAACDEGYAAIPYMvcAc6iq45IclyR7Z5+ZpwEAmOySW8y6+4Tu3trdW/fIXnOPAwCQZBcNMwCA\nEQkzAIBBbNowq6pnVNVn5p4DAGB7bdowS3JgkrvOPQQAwPbatGHW3S/o7pp7DgCA7bVpwwwAYNkI\nMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMA\ngEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBB\nCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgz\nAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCA\nQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEI\nMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBLE2YVdUvVtU5c88BAHBzWZow\nAwDY7HZKmFXV/lV1y53xXuv4nreuqr038nsCANycdjjMqmr3qjq6qk5Jcl6S+yyW36KqTqiq86vq\nm1X1V1W1dcXve1JVXVxVD62qM6rqW1X1/qo6ZNX7P7eqzlu89rVJ9ls1wo8lOW/xvY7c0T8HAMAo\n1h1mVXWPqnpJki8meUOSbxJQrTIAAAV3SURBVCX5kSSnVlUleWeSOyR5ZJL7Jjk1yfuq6qAVb7NX\nkl9J8pQkD0hyyyR/sOJ7PCbJf0/y/CSHJ/lskmevGuXkJI9P8p1J3l1VZ1XVr60OPACAZbFdYVZV\nt6qqZ1bVx5J8IsndkvxCktt191O7+9Tu7iQPSXJYkp/q7tO6+6zu/tUkZyd54oq33JLk6YvXfDLJ\ny5IctQi7JHlWkj/u7ld195ndfXyS01bO1N1Xdfefd/fjktwuyYsW3/9zVfWBqnpKVa3eynbtn+e4\nqjq9qk6/MpdvzyoAALjZbe8Ws/+Y5BVJLktyl+5+VHe/qbsvW/W6I5Lsk+SCxS7Ii6vq4iT3TPJ9\nK153eXd/dsXXX0myZ5LvWnx9aJKPrHrv1V9/W3d/o7v/V3c/JMn3J7ltkj9K8lPbeP0J3b21u7fu\nkb1u4I8NALBxtmzn605IcmWSn0lyRlW9Ncnrkry3u69e8brdknw1yQ+u8R7fWPH4qlXP9Yrfv25V\ntVemXadPyHTs2d9n2ur2th15PwCAOWxXCHX3V7r7+O6+a5KHJbk4yZ8k+VJVvbyqDlu89OOZtlZd\ns9iNufLX+euY69NJfmDVsut8XZMHVdWrMp188LtJzkpyRHcf3t2v6O6L1vE9AQBmte4tVN390e5+\nWpKDMu3ivEuSv6mqH0zyniQfSvK2qvrRqjqkqh5QVb++eH57vSLJsVX11Kq6c1X9SpL7r3rNE5L8\nZZL9kzwuyXd393/u7jPW+2cCABjB9u7KvJ7uvjzJm5O8uapuk+Tq7u6q+rFMZ1S+OsltMu3a/FCS\n167jvd9QVd+b5PhMx6y9PclvJXnSipe9N9PJB9+4/jsAACyfmk6m3HXtXwf0/euhc48BAOxC3tNv\n/lh3b1293C2ZAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAG\nIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHM\nAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAA\nBiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYh\nzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwA\nAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAG\nIcwAAAYhzAAABiHMAAAGsWXuAeZQVcclOS5J9s4+M08DADDZJbeYdfcJ3b21u7fukb3mHgcAIMku\nGmYAACMSZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBm\nAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAA\ngxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQ\nZgAAgxBmAACDEGYAAIOo7p57hllV1QVJvjD3HNtwYJIL5x5iCVlv62ed7RjrbcdYb+tnne2Ykdfb\n93T3rVcv3OXDbGRVdXp3b517jmVjva2fdbZjrLcdY72tn3W2Y5ZxvdmVCQAwCGEGADAIYTa2E+Ye\nYElZb+tnne0Y623HWG/rZ53tmKVbb44xAwAYhC1mAACDEGYAAIMQZgAAgxBmAACDEGYAAIP4/6uA\npsWgeBMjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkiV_y9ebHEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}