{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Eng_to_Hin_NMT_using_attention_v3_LSTM+newdata.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ngN8uPp_0pML"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGYU3Wh_0pIc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# NMT using attention\n",
        "## NOTICE : Attention layer is not yet implemented in keras library , which makes implementation of attention mechanism a lot difficult than other neural network models\n",
        "## We can't use keras's model class and do model.fit , instead we'll have to write optimizer, loss function, model layers and training code from scratch and combine them to create a workflow. This is equivalent to writing model.fit in normal neural network models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PUXXp2K0pIg",
        "colab_type": "code",
        "outputId": "cae2df50-6219-4ed8-cf92-899c9ba78f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4_-K5Jzrx-W",
        "colab_type": "text"
      },
      "source": [
        "# loading the 1st dataset as it contains more smaller sentences which might help in better leanring\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOTBKDZcr1et",
        "colab_type": "code",
        "outputId": "b857d802-cf38-49fb-803e-0fa678e109cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# download dataset\n",
        "!wget 'http://www.manythings.org/anki/hin-eng.zip'\n",
        "!unzip 'hin-eng.zip'\n",
        "lines = io.open('hin.txt', encoding='UTF-8').read().strip().split('\\n')\n",
        "path_to_file = 'hin.txt'\n",
        "\n",
        "eng_sent = []\n",
        "hin_sent = []\n",
        "for line in lines:\n",
        "  splits = line.split('\\t')\n",
        "  #removing extra stuffs \n",
        "  eng = splits[0]\n",
        "  hin = splits[1]\n",
        "  # remove punctutation and make lower case\n",
        "  eng = eng.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = eng.lower()\n",
        "  hin = hin.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = '<start> ' + eng + ' <end>'\n",
        "  hin = '<start> ' + hin + ' <end>'\n",
        "  eng_sent.append(eng)\n",
        "  hin_sent.append(hin)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-19 07:36:15--  http://www.manythings.org/anki/hin-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:3037::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 126449 (123K) [application/zip]\n",
            "Saving to: ‘hin-eng.zip.1’\n",
            "\n",
            "\rhin-eng.zip.1         0%[                    ]       0  --.-KB/s               \rhin-eng.zip.1        40%[=======>            ]  50.41K   135KB/s               \rhin-eng.zip.1       100%[===================>] 123.49K   221KB/s    in 0.6s    \n",
            "\n",
            "2020-03-19 07:36:16 (221 KB/s) - ‘hin-eng.zip.1’ saved [126449/126449]\n",
            "\n",
            "Archive:  hin-eng.zip\n",
            "replace hin.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em5rP-OalHSR",
        "colab_type": "text"
      },
      "source": [
        "## Download and process dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmkR-YLV0pIp",
        "colab_type": "code",
        "outputId": "33bab9fd-c7ab-458d-8155-67b6e772b88a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# download dataset\n",
        "!wget 'http://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/iitb_corpus_download/prunedCorpus.tar.gz'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-19 07:38:51--  http://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/iitb_corpus_download/prunedCorpus.tar.gz\n",
            "Resolving www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)... 103.21.127.130\n",
            "Connecting to www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)|103.21.127.130|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84265584 (80M) [application/x-gzip]\n",
            "Saving to: ‘prunedCorpus.tar.gz.2’\n",
            "\n",
            "prunedCorpus.tar.gz 100%[===================>]  80.36M  3.17MB/s    in 31s     \n",
            "\n",
            "2020-03-19 07:39:22 (2.61 MB/s) - ‘prunedCorpus.tar.gz.2’ saved [84265584/84265584]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "562vbIWW0pIw",
        "colab_type": "code",
        "outputId": "4746a2e4-6fe1-44d6-a760-188949585f9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ls\n",
        "# !unzip 'prunedCorpus.tar.gz'\n",
        "!tar -xvzf 'prunedCorpus.tar.gz'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_about.txt     hin.txt\t\t      prunedCorpus.tar.gz.2  sample_data\n",
            "hin-eng.zip    prunedCorpus.tar.gz    pruned_train.en\n",
            "hin-eng.zip.1  prunedCorpus.tar.gz.1  pruned_train.hi\n",
            "pruned_train.en\n",
            "pruned_train.hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoPVHC4ZjduR",
        "colab_type": "code",
        "outputId": "9ae25ae4-04d5-4a44-f1c1-3720014cf91f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_about.txt     hin.txt\t\t      prunedCorpus.tar.gz.2  sample_data\n",
            "hin-eng.zip    prunedCorpus.tar.gz    pruned_train.en\n",
            "hin-eng.zip.1  prunedCorpus.tar.gz.1  pruned_train.hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9ckuHLOfIv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines_hin = io.open('pruned_train.hi', encoding='UTF-8').read().strip().split('\\n')\n",
        "path_to_file = 'pruned_train.hi'\n",
        "\n",
        "lines_eng = io.open('pruned_train.en', encoding='UTF-8').read().strip().split('\\n')\n",
        "path_to_file = 'pruned_train.en'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGmDgx3efKu6",
        "colab_type": "code",
        "outputId": "583b15f1-8d2c-49d9-c5f0-7ac7ff43a36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "for i in range(5):\n",
        "  print(lines_eng[i],lines_hin[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We generally find this capacity being directed towards mundane objects and close relations like wives , children and friends .  प्रायः हम इस भंडार को लौकिक विषयों पर और पत्नी , पुत्र मित्र आदि सगे संबंधियों पर प्रवर्तित कर देते हैं । \n",
            "I ' m sure I ' ll make a careless mistake मुझे जनता हूँ की मैं इस वीडियो में किसी भी समय\n",
            "Display web browser help वेब ब्राउज़र मदद दिखाएँ\n",
            "It is out of His Mercy that He has made for you Night and Day , - that ye may rest therein , and that ye may seek of his Grace ; - and in order that ye may be grateful .  और उसने अपनी मेहरबानी से तुम्हारे वास्ते रात और दिन को बनाया ताकि तुम रात में आराम करो और दिन में उसके फज़ल व करम की तलाश करो और ताकि तुम लोग शुक्र करो\n",
            "The disease caused by nutritional deficiency of ascorbic acid , formaly occuring in people who had not been to sea results in circular spots , stripes or patches scattered over thighs , arms , and trunk .  एब्सकार्बिक अम्ल कुपोषण की कमी के कारण द्वारा होने वाली बीमारी जो आमतौर से लोगो में होता है जिसके कारण गोलाकार धब्बे , पट्टियों या जांघों , बांह और धड़ पर चकत्ते । \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZxcav5VlMsN",
        "colab_type": "code",
        "outputId": "11ec4c5e-cc07-480a-a9d8-73940a4051d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(lines_eng),len(lines_hin))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "788098 788098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5OXf1td0pI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(lines_hin)):\n",
        "  # splits = line.split('\\t')\n",
        "  #removing extra stuffs \n",
        "  eng = lines_eng[i]\n",
        "  hin = lines_hin[i]\n",
        "  # remove punctutation and make lower case\n",
        "  eng = eng.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = eng.lower()\n",
        "  hin = hin.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = '<start> ' + eng + ' <end>'\n",
        "  hin = '<start> ' + hin + ' <end>'\n",
        "  eng_sent.append(eng)\n",
        "  hin_sent.append(hin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mh6mOtu0pJd",
        "colab_type": "code",
        "outputId": "39dbf09f-80b3-4f83-f2d8-e47375d1514b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(eng_sent[1:10],hin_sent[1:10])\n",
        "print(len(eng_sent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<start> help <end>', '<start> jump <end>', '<start> jump <end>', '<start> jump <end>', '<start> hello <end>', '<start> hello <end>', '<start> cheers <end>', '<start> cheers <end>', '<start> got it <end>'] ['<start> बचाओ <end>', '<start> उछलो <end>', '<start> कूदो <end>', '<start> छलांग <end>', '<start> नमस्ते। <end>', '<start> नमस्कार। <end>', '<start> वाहवाह <end>', '<start> चियर्स <end>', '<start> समझे कि नहीं <end>']\n",
            "790876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh69CycGraxP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O4W676E8-_Y",
        "colab_type": "text"
      },
      "source": [
        "# code to encode dataset and create dictionary using keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRwYu__H0pJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdkAsOUD0pJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    # default filter value is '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    # an instance of tokenizer\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "    \n",
        "    #takes all sentences in lang and makes dictionary for it\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    \n",
        "    #encodes the text -> (text ->vector)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    #puts appropriate number of zeros after the sent\n",
        "    # if maxlen of sentences if 100 and any given sentence is of len 20, then\n",
        "    # it'll pad 80 zeros at the end\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "    # tensor is a vector of n*maxlen\n",
        "    # and lang_tokenizer is a dictionary mapping word with key \n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSlzBh30pJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YfOYE-k0pJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(inp_lang,targ_lang,load_dataset):\n",
        "    inp_lang = inp_lang[1:num_examples]\n",
        "    targ_lang = targ_lang[1:num_examples]\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mr8IlOCbcdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLmet1lA0pJ5",
        "colab_type": "text"
      },
      "source": [
        "# encoding datatset and creating dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvkZM620pJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 20000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(eng_sent,hin_sent,num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVhz5n6i0pKC",
        "colab_type": "code",
        "outputId": "5d04d0ab-79fb-488f-ca35-03b2823331cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(input_tensor.shape,target_tensor.shape)\n",
        "print(\"max length of input and output are : \", max_length_inp,max_length_targ )\n",
        "print('english dictionary is : ')\n",
        "print(inp_lang)\n",
        "print(\"hindi dictionary is : \")\n",
        "print(targ_lang)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19999, 426) (19999, 406)\n",
            "max length of input and output are :  426 406\n",
            "english dictionary is : \n",
            "<keras_preprocessing.text.Tokenizer object at 0x7f8f280ee898>\n",
            "hindi dictionary is : \n",
            "<keras_preprocessing.text.Tokenizer object at 0x7f8fa848d0b8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSTMdEJF0pKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05i8WUQT0pKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ucRpKUo0pKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrMEbrgJ0pKZ",
        "colab_type": "code",
        "outputId": "b08eca0d-6004-47b2-834e-049e6dbc9b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17999 17999 2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayaYd_fn0pKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #This is what our model is going to learn\n",
        "# def convert(lang, tensor):\n",
        "#     for t in tensor:\n",
        "#         if t!=0:\n",
        "#             print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "# print (\"Input Language; index to word mapping\")\n",
        "# convert(inp_lang, input_tensor_train[0])\n",
        "# print ()\n",
        "# print (\"Target Language; index to word mapping\")\n",
        "# convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRmaikr_0pKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2dB-PgW0pKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train) #training set size\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "#Creates a Dataset whose elements are slices of the given tensors.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "#Combines consecutive elements of this dataset into batches.\n",
        "#eg dataset = tf.data.Dataset.range(8) \n",
        "# dataset = dataset.batch(3) \n",
        "# list(dataset.as_numpy_iterator()) \n",
        "# [ array([0,1,2]), array([3,4,5]) , array([5,6,7])]\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2ZwrCC0pKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i7_WbqN0pK0",
        "colab_type": "code",
        "outputId": "e500118a-2ea6-413c-b18a-c7aaff2e2086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# so dataset loads 64 examples at a time for both input and output\n",
        "# its a pointer to point to next batch when needed.\n",
        "print(dataset)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((32, 426), (32, 406)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UaxPkM50pK6",
        "colab_type": "code",
        "outputId": "f67ab974-3772-4b20-a417-b99b78998edf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(32), Dimension(426)]),\n",
              " TensorShape([Dimension(32), Dimension(406)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmPlb5vH0pK_",
        "colab_type": "code",
        "outputId": "99a0b59b-9d7a-422f-997d-d1387be09e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(example_input_batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   2   12   28 ...    0    0    0]\n",
            " [   2 1182 5144 ...    0    0    0]\n",
            " [   2 4181 7202 ...    0    0    0]\n",
            " ...\n",
            " [   2   40  318 ...    0    0    0]\n",
            " [   2    8  210 ...    0    0    0]\n",
            " [   2  686 6957 ...    0    0    0]], shape=(32, 426), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frqAhmQV0pLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuLqs00r0pLK",
        "colab_type": "text"
      },
      "source": [
        "# Making the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmRIFAol0pLR",
        "colab_type": "text"
      },
      "source": [
        "**return_sequences** -> give hidden state for each time step  \n",
        "**return_state** -> gives (in case of lstm) [hidden state, hidden state, cell state for last time step].  \n",
        "**both of them** -> (in case of lstm)[hidden state for all time step, hidden state for last time step, cell state for last time step]  \n",
        "(in case of gru)[hidden state for all time steps, hidden state for final timestep]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Ygwhtw0pLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        #takes input and returns h and y\n",
        "        x = self.embedding(x)\n",
        "        output, state ,cell = self.lstm(x, initial_state = hidden)\n",
        "        encoder_states = [state,cell]\n",
        "        return output, encoder_states\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        state =  tf.zeros((self.batch_sz, self.enc_units))\n",
        "        return [state,state]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJzLv2P00pLZ",
        "colab_type": "code",
        "outputId": "5f496c4f-647f-416a-e682-cbc2ce799771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(vocab_inp_size)\n",
        "print(embedding_dim,units,BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24745\n",
            "256 1024 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddGof_wu0pLf",
        "colab_type": "code",
        "outputId": "fc3c51ab-8549-486c-ce96-eef836cbdddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# encoder is a class instance having \n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# not sure how this line is working..\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "sample_output, encoder_states = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden[0].shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (32, 426, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (32, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxbfkV_l0pLl",
        "colab_type": "code",
        "outputId": "c0ecdef2-083c-4f00-b117-dc6af8e2b5b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "print(\"shape of sample_hidden\",sample_hidden[0].get_shape())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of sample_hidden (32, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF56_kQJ0pLp",
        "colab_type": "text"
      },
      "source": [
        "# ATTENTION MECHANISAM  CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe_AOor30pLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        \n",
        "        query = query[0]\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size) \n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVjh8iKZ0pL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu8Fx0Iw0pL9",
        "colab_type": "code",
        "outputId": "cd16ad53-9f61-429d-c423-b1928e61a90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# this is just testing if attention is working or not\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (32, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (32, 426, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq0lelzP0pMB",
        "colab_type": "code",
        "outputId": "27f23aff-4048-4594-a0c7-6fefbcda0dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(sample_hidden)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: id=20, shape=(32, 1024), dtype=float32, numpy=\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: id=20, shape=(32, 1024), dtype=float32, numpy=\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Rh02IR0pMG",
        "colab_type": "text"
      },
      "source": [
        "## DECODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7HYsCtT0pMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        # enc_output is the list of all the hidden states\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "\n",
        "        output, hidden, cell = self.lstm(x)\n",
        "        decoder_states = [hidden,cell]\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, decoder_states, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngN8uPp_0pML",
        "colab_type": "text"
      },
      "source": [
        "### state is hidden state of gru for that time step\n",
        "### output and state are the same thing here , as this is only for one time step\n",
        "### output is :  Tensor(\"decoder_2/gru_5/transpose_1:0\", shape=(32, 1, 1024), dtype=float32)\n",
        "### state is :  Tensor(\"decoder_2/gru_5/while/Exit_3:0\", shape=(32, 1024), dtype=float32)\n",
        "notice the difference in shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE_RH93Q0pMN",
        "colab_type": "text"
      },
      "source": [
        "### testing if decoder class is working\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfVtqV_o0pMP",
        "colab_type": "code",
        "outputId": "640a0110-2a3b-4e43-85c3-1d3a4e7b48e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (32, 31298)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYK3knvn0pMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQQ7D8ZuA2bQ",
        "colab_type": "text"
      },
      "source": [
        "# OPTIMIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfa3quU-0pMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "'''\n",
        "In the snippet below, there is a single floating point value per example for\n",
        "`y_true` and `# classes` floating pointing values per example for `y_pred`.\n",
        "The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n",
        "`[batch_size, num_classes]`.\n",
        "'''\n",
        "#Computes the crossentropy loss between the labels and predictions.\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "# only considering cross entropy for incorrectly classified samples.\n",
        "def loss_function(real, pred):\n",
        "    # tf.math.logical - Returns the truth value of NOT x element-wise.\n",
        "    # tf.math.equal - Returns the truth value of (x == y) element-wise.\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    # loss as defined in the image above\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zpVkGjN0pMf",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6P1MX520pMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B241DBs60pMl",
        "colab_type": "code",
        "outputId": "39ff824c-744b-4bea-e247-f18919cbe69c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "for (batch, (inp, targ)) in enumerate( dataset.take(steps_per_epoch) ):\n",
        "    print(targ)\n",
        "    print(targ.shape)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[    1   209    13 ...     0     0     0]\n",
            " [    1 11161  3601 ...     0     0     0]\n",
            " [    1   748   599 ...     0     0     0]\n",
            " ...\n",
            " [    1    59  1431 ...     0     0     0]\n",
            " [    1    28  4965 ...     0     0     0]\n",
            " [    1   734 18854 ...     0     0     0]], shape=(32, 406), dtype=int32)\n",
            "(32, 406)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hezjeujC0pMs",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daxa8Bz60pM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    '''\n",
        "    inp is encoded input sentence \n",
        "    targ is encoded output sentence(batchsize,len_of_target_word)\n",
        "    enc_hidden is zero vector of shape (batchsize,units)\n",
        "    '''\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        #recoder or tracks variable values\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # targ_lang.word_index['<start>'] - returns dictionary value of start token\n",
        "        # dec_input is batch_size\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            #enc_output is list of all hidden states of encoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            \n",
        "            # this is simply the loss between t th predicted word and its prediction\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            # expand_dims is to change dimention such that it can act as timesteps\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    \n",
        "    # explanationn of this part https://www.tensorflow.org/guide/effective_tf2\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    \n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NROWVZyG0pM6",
        "colab_type": "text"
      },
      "source": [
        "## operation code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMWEDCKS0pM7",
        "colab_type": "code",
        "outputId": "34dcbb78-ba1c-48d2-eabf-7e3b42ffc9c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # returns a zero vector of shape(batch_size,units)\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    # dataset is tf.data.Dataset() object.\n",
        "    for (batch, (inp, targ)) in enumerate( dataset.take(steps_per_epoch) ):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MvHbnXxOzUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucOohbQEO1jd",
        "colab_type": "text"
      },
      "source": [
        "## Evaluator\n",
        "The evaluate function is similar to the training loop, except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA4CqPHaD6gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(eng):\n",
        "  eng = eng.translate(str.maketrans('', '', string.punctuation))\n",
        "  eng = eng.lower()\n",
        "  eng = '<start> ' + eng + ' <end>'\n",
        "  return eng"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZfulsHr0pNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  # this is for the heat map\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  #convert input to its embeddings\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_inp,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # we'll keep appending the predicted word in this\n",
        "  result = ''\n",
        "\n",
        "  # got all the hidden states and last encoder hidden state\n",
        "  hidden = [tf.zeros((1, units)),tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # for first timestep decoder hidden state = encoder hidden state\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  # keep feeding words in decoder for no of words in target sentence\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                          dec_hidden,\n",
        "                                                          enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    # max value from probability of all words in target sentence\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    # print(predictions.shape,predictions)\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jauhunjBZ_V_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stinglXdax8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSjdil8ta9JD",
        "colab_type": "text"
      },
      "source": [
        "# Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAV5x7eAa3Uq",
        "colab_type": "code",
        "outputId": "7f659313-310e-4aeb-b13d-05c59351d775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fae7a7f47f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf3Fur9GbCp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrJ4hztybDqP",
        "colab_type": "text"
      },
      "source": [
        "## checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBTdRi0WEUt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8vEmQFa49V",
        "colab_type": "code",
        "outputId": "3e709ed2-8a40-4420-d54c-ddb253405019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate('I will get back to you')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> i will get back to you <end>\n",
            "Predicted translation: मैं तुम्हारे साथ आऊँगा। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkiV_y9ebHEx",
        "colab_type": "code",
        "outputId": "5cda8216-88b7-47ad-9ecf-51646ba3fd2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate('This cat is nice')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> this cat is nice <end>\n",
            "Predicted translation: यह लिफ़्ट छटी मंज़िल तक ही है। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40qIl6PXCUbk",
        "colab_type": "code",
        "outputId": "97d68677-96e0-4cdc-ec12-fd3901cb9fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(u'i will not come')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> i will not come <end>\n",
            "Predicted translation: मैं तुम्हारे साथ आऊँगा। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RrASOTFHNeG",
        "colab_type": "code",
        "outputId": "92a1e241-c43f-40cc-a596-489fc6348ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(u'men are playing')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> men are playing <end>\n",
            "Predicted translation: वे हमारे साथ आएगा। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYd2qzwVHUuM",
        "colab_type": "code",
        "outputId": "26876bab-7daa-4836-8422-d374bc773487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(u'school is closed')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> school is closed <end>\n",
            "Predicted translation: स्कूल अप्रैल में बोलिए। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4YDk2xrHimX",
        "colab_type": "code",
        "outputId": "29548c3a-6a54-464f-f2ae-466c3bd72f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(u'why does this happen to me')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> why does this happen to me <end>\n",
            "Predicted translation: तुम क्यों नहीं हो <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St8LLzV2PsgI",
        "colab_type": "code",
        "outputId": "76e81fc4-987b-4dc2-e71d-c3a051679524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate('this is not working')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> this is not working <end>\n",
            "Predicted translation: यह भी यकीन है। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyovofYfPzlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}